# -*- coding: utf-8 -*-
"""COMP5413_Software_Project_5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ANs4JBZZYOXHCOJxbWo7Ko4WFhxe7h_U
"""

!ls /content/drive/MyDrive/Software_data/code_metrics/

! ls /content/drive/MyDrive/Software_data/code_metrics/csv

! ls /content/drive/MyDrive/Software_data/code_metrics/model

! ls /content/drive/MyDrive/Software_data/code_metrics/py

! ls /content/drive/MyDrive/Software_data/code_metrics/xml

"""# 0\. Unzip zip files"""

import os, zipfile, shutil
      
py_path = "/content/drive/MyDrive/Software_data/code_metrics/py"
py_path = MAIN_PATH + "code_metrics/py"
for zip in os.listdir(py_path):
  zip_path = py_path + "/" + zip
  if ".zip" not in zip_path:
    shutil.rmtree(zip_path, ignore_errors=True)
    print("{} deleted!".format(zip_path))

for zip in os.listdir(py_path):
  zip_path = py_path + "/" + zip
  zip_file = zipfile.ZipFile(zip_path)
  zip_file.extractall(py_path) 
  print("{} unzipped!".format(zip_path))
print()

xml_path = "/content/drive/MyDrive/Software_data/code_metrics/xml" 
xml_path = MAIN_PATH + "code_metrics/xml"
for zip in os.listdir(xml_path):
  zip_path = xml_path + "/" + zip
  if ".zip" not in zip_path:
    shutil.rmtree(zip_path, ignore_errors=True) 
    print("{} deleted!".format(zip_path))

for zip in os.listdir(xml_path):
  zip_path = xml_path + "/" + zip
  zip_file = zipfile.ZipFile(zip_path)
  zip_file.extractall(xml_path) 
  print("{} unzipped!".format(zip_path))

"""# 1\. Preparation"""

from google.colab import drive
drive.mount('/content/drive')

MAIN_PATH = "/content/drive/MyDrive/Software_data/"

"""## 1\.1 Packages"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
from collections import Counter
import pickle

from sklearn import preprocessing
from sklearn.utils import shuffle
from imblearn.over_sampling import SMOTE  # not available in sklearn 0.24.2
from sklearn.model_selection import train_test_split, StratifiedKFold

from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, LabelBinarizer
from sklearn.svm import SVC
from tensorflow import keras
from tensorflow.keras.models import Model, load_model, Sequential
from tensorflow.keras.layers import Dense

from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,roc_auc_score,classification_report,confusion_matrix

# from sklearn.feature_selection import SequentialFeatureSelector => not available in sklearn 0.22.2
from mlxtend.feature_selection import SequentialFeatureSelector as SFS

import os

from sklearn.metrics.pairwise import cosine_similarity

import warnings   # to disable warning
warnings.filterwarnings('ignore')

"""## 1.2 Methods to use"""

# for plotting confusion matrix
def plot_confusion_matrix(cm,
                          target_names,
                          title='Confusion matrix',
                          cmap=None,
                          normalize=True):
  
    import matplotlib.pyplot as plt
    import numpy as np
    import itertools

    accuracy = np.trace(cm) / float(np.sum(cm))
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('Blues')

    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    if target_names is not None:
        tick_marks = np.arange(len(target_names))
        plt.xticks(tick_marks, target_names, rotation=45)
        plt.yticks(tick_marks, target_names)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="gray" if cm[i, j] > thresh else "black")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="gray" if cm[i, j] > thresh else "black")


    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
    plt.show()

# method to split the dataset into train and test sets
def split_dataset_tr_te(X,y,test_percentage=0.2):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_percentage, stratify = y)
    
    # verify the class distribution
    print("class 0:1 in y is       : {}:{}".format(Counter(list(y))[0]/y.shape[0], 1-Counter(list(y))[0]/y.shape[0]))
    print("class 0:1 in y_train is : {}:{}".format(Counter(list(y_train))[0]/y_train.shape[0], 1-Counter(list(y_train))[0]/y_train.shape[0]))
    print("class 0:1 in y_test is  : {}:{}".format(Counter(list(y_test))[0]/y_test.shape[0], 1-Counter(list(y_test))[0]/y_test.shape[0]))
    print()
    
    return X_train, X_test, y_train, y_test

# method to train models
def train_model(X_train,y_train,X_test,y_test,save_path = "./",data_type="le_smote",model_type='dt'):
    # save the validation accuracy of each fold
    fold_acc = []
    fold_model = []

    time_st = time.time()

    skf = StratifiedKFold(n_splits=5)
    skf_tr_va_sets = skf.split(X_train, y_train)

    print("Training")
    print("-"*80)

    # loop through each fold
    for idx_f, (idx_tr, idx_va) in enumerate(skf_tr_va_sets):
        # tr_X = X_train.iloc[idx_tr]
        # tr_y = y_train.iloc[idx_tr]
        # va_X = X_train.iloc[idx_va]
        # va_y = y_train.iloc[idx_va]
        tr_X = X_train[idx_tr]
        tr_y = y_train[idx_tr]
        va_X = X_train[idx_va]
        va_y = y_train[idx_va]

        if model_type == 'dt':
            model = tree.DecisionTreeClassifier(max_depth=None)
        elif model_type == 'rf':
            model = RandomForestClassifier(max_depth=None, random_state=0)
        elif model_type == 'knn':
            model = KNeighborsClassifier(n_neighbors=3)
        elif model_type == 'nb':
            model = MultinomialNB()
        elif model_type == 'lr':
            model = LogisticRegression(random_state=0)
        elif model_type == 'lda':
            model = LinearDiscriminantAnalysis()
        elif model_type == 'qda':
            model = QDA()
        elif model_type == 'svm':
            model = make_pipeline(StandardScaler(), SVC(gamma='auto'))
        elif model_type == 'mlp':
            model = Sequential()
            model.add(Dense(32, input_dim = tr_X.shape[1], activation='relu', kernel_initializer='he_normal'))
            model.add(Dense(1, activation='sigmoid'))
            model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

            history = model.fit(tr_X, tr_y, 
                                validation_data = (va_X, va_y), 
                                epochs=300, 
                                batch_size=256,
                                verbose=0) 
            plt.plot(history.history['accuracy'])
            plt.plot(history.history['val_accuracy'])
            plt.title('fold {} - accuracy'.format(idx_f+1))
            plt.ylabel('accuracy')
            plt.xlabel('epoch')
            plt.legend(['train', 'validation'], loc='upper left')
            plt.show()
            # summarize history for loss
            plt.plot(history.history['loss'])
            plt.plot(history.history['val_loss'])
            plt.title('fold {} - loss'.format(idx_f+1))
            plt.ylabel('loss')
            plt.xlabel('epoch')
            plt.legend(['train', 'validation'], loc='upper left')
            plt.show()  

        elif model_type == 'nn':
            model = Sequential()
            model.add(Dense(32, input_dim = tr_X.shape[1], activation='relu'))
            model.add(Dense(16, activation='relu'))
            model.add(Dense(8, activation='relu'))
            model.add(Dense(4, activation='relu'))
            model.add(Dense(2, activation='softmax'))
            model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

            history = model.fit(tr_X, tr_y, 
                                validation_data = (va_X, va_y), 
                                epochs=300, 
                                batch_size=256,
                                verbose=0) 
            # list all data in history
            # print(history.history.keys())
            # summarize history for accuracy
            plt.plot(history.history['accuracy'])
            plt.plot(history.history['val_accuracy'])
            plt.title('fold {} - accuracy'.format(idx_f+1))
            plt.ylabel('accuracy')
            plt.xlabel('epoch')
            plt.legend(['train', 'validation'], loc='upper left')
            plt.show()
            # summarize history for loss
            plt.plot(history.history['loss'])
            plt.plot(history.history['val_loss'])
            plt.title('fold {} - loss'.format(idx_f+1))
            plt.ylabel('loss')
            plt.xlabel('epoch')
            plt.legend(['train', 'validation'], loc='upper left')
            plt.show()    
        
        if model_type not in ['mlp','nn']:
            model = model.fit(tr_X, tr_y)
        va_y_pred = model.predict(va_X)
        # print(va_y_pred)
        if model_type == 'mlp':
            va_y_pred[va_y_pred>=0.5] = 1
            va_y_pred[va_y_pred<0.5] = 0
        elif model_type == 'nn':
            va_y_pred = [np.argmax(y_p) for y_p in va_y_pred]
        
        fold_acc.append(accuracy_score(va_y, va_y_pred))
        fold_model.append(model)

    avg_acc = sum(fold_acc)/len(fold_acc)
    abs_acc = [abs(_-avg_acc) for _ in fold_acc]
    print("Fold accuracy: {}\nAverage accuracy: {}\nSelected Model accuracy: {}".format(fold_acc, avg_acc, fold_acc[abs_acc.index(min(abs_acc))]))

    # save the model
    model_to_save = fold_model[abs_acc.index(min(abs_acc))]       # save the model with the performance most close to the average
    # model_to_save = fold_model[fold_acc.index(max(fold_acc))]     # save the model with the highest performance
    if model_type not in ['mlp','nn']:    # save_path = "./",data_type="le_smote",model_type='dt'
      model_path = save_path + "model_" + data_type + "_" + model_type + ".pkl"
      with open(model_path, 'wb') as file:
        pickle.dump(model_to_save, file)
    else:
      model_path = save_path + "model_" + data_type + "_" + model_type + ".h5"
      model_to_save.save(model_path)
    print("\nModel saved at {}\n".format(model_path))

    print("Training time (sec): {:0.9f}\n".format(time.time()-time_st))

    print("Testing")
    print("-"*80)
    # load and test the model

    if model_type not in ['mlp','nn']:
      with open(model_path, 'rb') as file:
        model = pickle.load(file)
    else:
      model = load_model(model_path)
    print("\nModel loaded from {}\n".format(model_path))  
    y_test_pred = model.predict(X_test)
    if model_type == 'mlp':
        y_test_pred[y_test_pred>=0.5] = 1
        y_test_pred[y_test_pred<0.5] = 0
        y_test_pred = y_test_pred.astype(np.uint8)
    elif model_type == 'nn':
        y_test_pred = [np.argmax(y_p) for y_p in y_test_pred]

    # y_true = list(y_test.iloc[:,0])
    y_true = list(y_test)
    if model_type not in ['mlp','nn']:
        y_test_pred = y_test_pred.tolist()
    elif model_type == 'mlp':
        y_test_pred = y_test_pred.flatten().tolist()

    # save the true and prediction lists
    np.save(save_path + "true_" + data_type + "_" + model_type + ".npy",y_true)
    print(save_path + "true_" + data_type + "_" + model_type + ".npy saved.")
    np.save(save_path + "pred_" + data_type + "_" + model_type + ".npy",y_test_pred)
    print(save_path + "pred_" + data_type + "_" + model_type + ".npy saved.")

    print("True       : {} ...".format(y_true[0:20]))
    print("Prediction : {} ...".format(y_test_pred[0:20]))
    
    print("Accuracy score             :",accuracy_score(y_true, y_test_pred))
    print("F1-score                   :",f1_score(y_true, y_test_pred))
    print("Precision score            :",precision_score(y_true, y_test_pred))
    print("Recall score               :",recall_score(y_true, y_test_pred))
    print("Auc score                  :",roc_auc_score(y_true, y_test_pred))
    print("Classification report      :\n",classification_report(y_true, y_test_pred))
    print("Confusion Matrix           :\n",confusion_matrix(y_true, y_test_pred))

    metric_list = [accuracy_score(y_true, y_test_pred),
                    f1_score(y_true, y_test_pred),
                    precision_score(y_true, y_test_pred),
                    recall_score(y_true, y_test_pred),
                    confusion_matrix(y_true, y_test_pred).item((0,0)),
                    confusion_matrix(y_true, y_test_pred).item((0,1)),
                    confusion_matrix(y_true, y_test_pred).item((1,0)),
                    confusion_matrix(y_true, y_test_pred).item((1,1))]

    # save the metrics for the testing result
    np.save(save_path + "metric_" + data_type + "_" + model_type + ".npy",metric_list)
    print(save_path + "metric_" + data_type + "_" + model_type + ".npy saved.")

    # plot confusion matrix
    cm = confusion_matrix(y_true, y_test_pred)
    plot_confusion_matrix(cm, normalize = True, target_names = ["not cloned","cloned"], title = "Confusion Matrix")

# method to get the index of the last character in a string when there are multiple occurrence of this character
def get_last_index(str_in, ch_in):
    if ch_in not in str_in:
        return None
    for idx in range(len(str_in)):
        if str_in[idx]==ch_in and ch_in not in str_in[idx+1:]:
            return idx
            
print(get_last_index('blueprintapp.apps.admin.index()','?'))    
print(get_last_index('blueprintapp.apps.admin.index()','.'))
print(get_last_index('blueprintapp.apps.admin.index()','a'))
print(get_last_index('flask-main/tests/test_user_error_handler.py','/'))

# metod to get the last index of a character or a string in another string
def get_last_index_ch_st(str_in, ch_st_in):
    if ch_st_in not in str_in:
        return None
    for idx in range(len(str_in)):
        if str_in[idx:idx+len(ch_st_in)]==ch_st_in and ch_st_in not in str_in[idx+len(ch_st_in)+1:]:
            # print(str_in[idx:idx+len(ch_st_in)], str_in[idx+len(ch_st_in)+1:])
            return idx

str1 = "def func(self,x=None, y=1, z=None, l=None) :"
print(str1)
if str1[get_last_index_ch_st(str1,"=None")+len("=None"):][0]==")":
  # replace only the "=None" in front of the last one
  print("True")
  str1 = str1[:get_last_index_ch_st(str1,"=None")].replace("=None","") + str1[get_last_index_ch_st(str1,"=None"):]
  print(str1)
print()

def get_first_uppercase_index(str_in):
    for idx in range(len(str_in)):
        if str_in[idx].isupper():
            return idx
    return None

str1 = "flask.blueprints.Blueprint.add_app_template_filter(self,f,name)"
idx = get_last_index(str1,".")
str1 = str1[:idx]
str1 = str1[:get_first_uppercase_index(str1)]if get_first_uppercase_index(str1) != None else str1
str1 = str1.replace("."," ").split()    
print(str1)

def all_list_string_in_one_string_line(str_line, str_list):
    for s in str_list:
        if s not in str_line:
            return False
    return True
print(all_list_string_in_one_string_line("Hello, world!", ["Hello"]))
print(all_list_string_in_one_string_line("Hello, world!", ["Hello","world"]))
print(all_list_string_in_one_string_line("Hello, world!", ["Hello","world","and"]))

def any_list_string_in_one_string_line(str_line, str_list):
    for s in str_list:
        if s in str_line:
            return True
    return False
print(any_list_string_in_one_string_line("Hello, world!", ["Hello"]))
print(any_list_string_in_one_string_line("Hello, world!", ["Hello","world"]))
print(any_list_string_in_one_string_line("Hello, world!", ["and"]))

# method to parse the function names
def parse_function_name_line(str_line_tmp):
    # print(str_line_tmp)
    str_line_tmp = str_line_tmp.replace(" ","").replace("\n","").replace(",)",")")
    str_1 = ""   # in case there are "=", "*", and "**" to be processed, store part1 of the string
    str_2 = ""   # store part2 of the string
    # print(str_line_tmp)
    if "=" not in str_line_tmp:
        str_line_tmp = str_line_tmp.replace("*","")
        return str_line_tmp[:-1]
    # for the case when "=" and "[" are in the line, replace "[  ]" with 'dummy'
    # def __init__(self, choices = []) :
    # def test_relative_path(self, path = ['fixtures', 'absolute.json']) :
    if "=" in str_line_tmp and "[" in str_line_tmp:
        str_line_tmp = str_line_tmp.replace(str_line_tmp[str_line_tmp.index("["):str_line_tmp.index("]")+1],'dummy')
    # for the case when "= (" are in the line, replace "= (  )" with '= dummy'
    # coords(self, query, ordering = ('longitude', 'latitude')) :
    if "=(" in str_line_tmp:
        str_line_tmp = str_line_tmp.replace(str_line_tmp[str_line_tmp.index("=("):str_line_tmp.index(")")+1],'=dummy')
    if "=" in str_line_tmp and "*" in str_line_tmp:
      # for a special case 'def file_move_safe(old_file_name, new_file_name, chunk_size = 1024 * 64, allow_overwrite = False) :'
      # -> django.core.files.move.file_move_safe(old_file_name,new_file_name,chunk_size,allow_overwrite)
      # decide whether there is "=" behind "*", if yes replace this "*" as ""
      if "=" in str_line_tmp[str_line_tmp.index("*")+1:]:
        str_line_tmp = str_line_tmp.replace("*","")
      else:
        # this is the case of "def authenticate(request = None, ** credentials) :"
        # -> django.contrib.auth.authenticate(request,credentials)

        # or the case of "def _response(self, template = 'foo', * args, ** kwargs) :"
        # -> template_tests.test_response.TemplateResponseTest._response(self,template,args,kwargs)

        str_1 = str_line_tmp[:str_line_tmp.index("*")]
        str_1 = str_1[:get_last_index(str_1,",")] + ")"
        str_2 = str_line_tmp[str_line_tmp.index("*"):]
        str_line_tmp = str_1

    while "=" in str_line_tmp:
        # print("Before processing     :", str_line_tmp)
        str_line_tmp_idx1 = str_line_tmp.index("=")
        str_line_tmp_new = str_line_tmp[str_line_tmp_idx1+1:]
        # print("String New            :", str_line_tmp_new)
        if "," in str_line_tmp_new and "=" in str_line_tmp_new:
            str_line_tmp_idx2 = str_line_tmp_new.index(",")
            str_to_replace = str_line_tmp[str_line_tmp_idx1:str_line_tmp_idx1+str_line_tmp_idx2+1]
            # print("string to replace :", str_to_replace)
            # to avoid mistake for "format(number, decimal_sep, decimal_pos = None, grouping = 0, thousand_sep = '',force_grouping = False, use_l10n = None) :"
            # -> "django.utils.numberformat.format(number,decimal_sep,decimal_pos,grouping,thousand_sep,force_grouping,use_l10n)"
            # do not replace "= None" if it's in front of ")"
            if str_line_tmp[get_last_index_ch_st(str_line_tmp,str_to_replace)+len(str_to_replace):][0]==")":
              # replace only the str_to_replace in front of the last one
              str_line_tmp = str_line_tmp[:get_last_index_ch_st(str_line_tmp,str_to_replace)].replace(str_to_replace,"") + str_line_tmp[get_last_index_ch_st(str_line_tmp,str_to_replace):]
            else:
              # replace all the str_to_replace
              str_line_tmp = str_line_tmp.replace(str_to_replace,"")
            # print("After processing  : ", str_line_tmp)
        if "," not in str_line_tmp_new and ")" in str_line_tmp_new:
            str_line_tmp_idx2 = str_line_tmp_new.index(")")
            str_to_replace = str_line_tmp[str_line_tmp_idx1:str_line_tmp_idx1+str_line_tmp_idx2+1]
            # print("string to replace :", str_to_replace)
            str_line_tmp = str_line_tmp.replace(str_to_replace,"")
            # print("After processing  : ", str_line_tmp)
    if str_2 == "":
      return str_line_tmp[:-1]
    else:
      str_2 = str_2.replace("*","")
      return str_line_tmp[:-1] + "," + str_2[:-1]

str_line = "github_link(name, rawtext, text, lineno, inliner, options = None, content = 'abc', content2='  abcde') :"
print(parse_function_name_line(str_line))
str_line = "test_error_handler_no_match(app, client) :"
print(parse_function_name_line(str_line))
str_line = 'file_move_safe(old_file_name, new_file_name, chunk_size = 1024 * 64, allow_overwrite = False) :'
print(parse_function_name_line(str_line))
str_line = "authenticate(request = None, ** credentials) :"
print(parse_function_name_line(str_line))
str_line = "_response(self, * args) :"
print(parse_function_name_line(str_line))
str_line = "_response(self, * args) :"
print(parse_function_name_line(str_line))
str_line = "_response(template = 'foo', * args, ** kwargs) :"
print(parse_function_name_line(str_line))
str_line = "_response(self, template = 'foo', * args, ** kwargs) :"
print(parse_function_name_line(str_line))
str_line = "test_relative_path(self, path = ['fixtures', 'absolute.json']) :"
print(parse_function_name_line(str_line))
str_line = "def new_file(self, field_name, file_name, content_type, content_length, charset = None,content_type_extra = None,) :"
print(parse_function_name_line(str_line))
str_line = "format(number, decimal_sep, decimal_pos = None, grouping = 0, thousand_sep = '',force_grouping = False, use_l10n = None) :"
print(parse_function_name_line(str_line))
str_line = "coords(self, query, ordering = ('longitude', 'latitude')) :"
print(parse_function_name_line(str_line))

"""# 2\. Flask

## 2.1 Extracting Data

### 2.1.1 Load code metrics in the CSV file exported from understand
"""

code_metric = MAIN_PATH + "code_metrics/csv/flask-main.csv"
code_metric_dataset = pd.read_csv(code_metric)
code_metric_dataset.head()

"""### 2.1.2 Keep only functions"""

code_metric_function = code_metric_dataset[code_metric_dataset['Kind']=='Function']
code_metric_function.head()

code_metric_function = code_metric_function[['Name', 
                                             'CountLine', 
                                             'CountLineBlank', 
                                             'CountLineCode', 
                                             'CountLineCodeDecl', 
                                             'CountLineCodeExe', 
                                             'CountLineComment', 
                                             'CountPath', 
                                             'CountPathLog', 
                                             'CountStmt', 
                                             'CountStmtDecl', 
                                             'CountStmtExe', 
                                             'Cyclomatic', 
                                             'CyclomaticModified', 
                                             'CyclomaticStrict', 
                                             'Essential', 
                                             'MaxNesting', 
                                             'RatioCommentToCode']]
code_metric_function.head()

"""### 2.1.3 Add 'function_location', 'StartLine', 'EndLine' to CSV for labelling by XML later"""

code_metric_function_2 = code_metric_function
code_metric_function_2['function_location']=['not_assigned' for _ in range(code_metric_function.shape[0])]
code_metric_function_2['StartLine']=[-1 for _ in range(code_metric_function.shape[0])]
code_metric_function_2['EndLine']=[-1 for _ in range(code_metric_function.shape[0])]
code_metric_function_2 = code_metric_function_2[['function_location',
                                                 'Name',
                                                 'StartLine',
                                                 'EndLine',
                                                 'CountLine', 
                                                 'CountLineBlank', 
                                                 'CountLineCode', 
                                                 'CountLineCodeDecl', 
                                                 'CountLineCodeExe', 
                                                 'CountLineComment', 
                                                 'CountPath', 
                                                 'CountPathLog', 
                                                 'CountStmt', 
                                                 'CountStmtDecl', 
                                                 'CountStmtExe', 
                                                 'Cyclomatic', 
                                                 'CyclomaticModified', 
                                                 'CyclomaticStrict', 
                                                 'Essential', 
                                                 'MaxNesting', 
                                                 'RatioCommentToCode']]
code_metric_function_2.head()

"""### 2.1.4 Match understand CSV and NiCad XML functions by Case 1, 2, 3"""

# look for the function names in the dataframe, if there is no duplicated name, assign the values to the dataframe row

# case 1 - the function in the dataframe has no replicated records, totally matched by function names
# the format of the function name in the dataframe is py_file_name.x.func_name

# parse the functions.xml file to locate the functions
function_xml_path = MAIN_PATH + "code_metrics/xml/flask/type1_flask-main_functions.xml"

f = open(function_xml_path, "r", encoding='utf-8')
lines = f.readlines()
f.close()

function_in_xml = [[], # store the py file location
                   [], # start line
                   [], # end line
                   []] # function name after processing

for idx_ln in range(len(lines)):
    ln = lines[idx_ln]
    if "<source file=" in ln:
        idx_st = ln.index("flask-main")
        idx_ed = ln.index(".py")
        py_path = ln[idx_st:idx_ed+len(".py")]
        # print(py_path)
        idx_st = ln.index('startline="')
        ln = ln[idx_st+len('startline="'):]
        start_line = ln[:ln.index('"')]
        # print(start_line)
        idx_st = ln.index('endline="')
        ln = ln[idx_st+len('endline="'):]
        end_line = ln[:ln.index('"')]
        # print(end_line)
        ln = lines[idx_ln+1]
        ln = ln[ln.index("def ")+len("def "):]
        func_name = parse_function_name_line(ln)
        # print(func_name)
        function_in_xml[0].append(py_path)
        function_in_xml[1].append(int(start_line))
        function_in_xml[2].append(int(end_line))
        function_in_xml[3].append(func_name)    
        
print("{} functions found in the .xml file".format(len(function_in_xml[0])))
print()
print("The first 5 records are:")
for i in range(5):
    print(function_in_xml[0][i], function_in_xml[1][i], function_in_xml[2][i], function_in_xml[3][i])
print()
 
# check how many rows not assigned before processing
print(Counter(list(code_metric_function_2['function_location'])))
print()

for idx in range(len(function_in_xml[3])):
    py_path = function_in_xml[0][idx]
    py_file_name = py_path[get_last_index(py_path,'/')+1:py_path.index(".py")]
    func_name = function_in_xml[3][idx].replace('(','\(').replace(')','\)').replace('*','\*')
    
    # print(repr(py_file_name), repr(func_name))
    
    # look for the dataframe rows that contain both py_file_name and func_name
    dataset_tmp = code_metric_function_2[(code_metric_function_2['Name'].str.contains(py_file_name)) & 
                                         (code_metric_function_2['Name'].str.contains(func_name)) &
                                         (code_metric_function_2['function_location']=='not_assigned')]
    # print(dataset_tmp)
    if dataset_tmp.shape[0] == 1: 
        code_metric_function_2.loc[dataset_tmp.index,'function_location'] = function_in_xml[0][idx]
        code_metric_function_2.loc[dataset_tmp.index,'StartLine'] = function_in_xml[1][idx]
        code_metric_function_2.loc[dataset_tmp.index,'EndLine'] = function_in_xml[2][idx]
    elif dataset_tmp.shape[0] == 0:
        print("Not found, rows have {} and {}".format(py_file_name, function_in_xml[3][idx]))
    else:
        print("Multiple rows have {} and {}".format(py_file_name, function_in_xml[3][idx]))

# check how many rows not changed
print()
print(Counter(list(code_metric_function_2['function_location'])))

for idx in range(len(function_in_xml[3])):
    py_path = function_in_xml[0][idx]
    py_file_name = py_path[get_last_index(py_path,'/')+1:py_path.index(".py")]
    func_name = function_in_xml[3][idx].replace('(','\(').replace(')','\)').replace('*','\*')
    
    # print(repr(py_file_name), repr(func_name))
    
    # look for the dataframe rows that contain both py_file_name and func_name
    dataset_tmp = code_metric_function_2[(code_metric_function_2['Name'].str.contains(py_file_name)) & 
                                         (code_metric_function_2['Name']==func_name) &
                                         (code_metric_function_2['function_location']=='not_assigned')]
    # print(dataset_tmp)
    if dataset_tmp.shape[0] == 1: 
        code_metric_function_2.loc[dataset_tmp.index,'function_location'] = function_in_xml[0][idx]
        code_metric_function_2.loc[dataset_tmp.index,'StartLine'] = function_in_xml[1][idx]
        code_metric_function_2.loc[dataset_tmp.index,'EndLine'] = function_in_xml[2][idx]
    elif dataset_tmp.shape[0] == 0:
        print("Not found, rows have {} and {}".format(py_file_name, function_in_xml[3][idx]))
    else:
        print("Multiple rows have {} and {}".format(py_file_name, function_in_xml[3][idx]))

# check how many rows not changed
print()
print(Counter(list(code_metric_function_2['function_location'])))

code_metric_function_2.head()

# case 2 - the function is nested in another function
# the format of the function name in the dataframe is py_file_name.func1_name.func2_name

# parse the functions.xml file to locate the functions
function_xml_path = MAIN_PATH + "code_metrics/xml/flask/type1_flask-main_functions.xml"
f = open(function_xml_path, "r", encoding='utf-8')
lines = f.readlines()
f.close()

function_in_xml = [[], # store the py file location
                   [], # start line of func2
                   [], # end line of func2
                   [], # func1 name after processing
                   []] # func2 name after processing

for idx_ln in range(len(lines)):
    ln = lines[idx_ln]
    if "<source file=" in ln:
        idx_st = ln.index("flask-main")
        idx_ed = ln.index(".py")
        py_path = ln[idx_st:idx_ed+len(".py")]
        # print(py_path)
        idx_st = ln.index('startline="')
        ln = ln[idx_st+len('startline="'):]
        start_line = ln[:ln.index('"')]
        # print("start_line_1",start_line)
        idx_st = ln.index('endline="')
        ln = ln[idx_st+len('endline="'):]
        end_line = ln[:ln.index('"')]
        # print("end_line_1",end_line)
        ln = lines[idx_ln+1]
        ln = ln[ln.index("def ")+len("def "):]
        func_1_name = parse_function_name_line(ln)
        # print(func_name)
        
        # obtain code segment of func1
        idx_ln_1 = idx_ln + 1
        code_1 = ""
        while "</source>" not in lines[idx_ln_1]: 
            code_1 += lines[idx_ln_1]
            idx_ln_1 += 1
        code_1 = code_1.replace("\n","").replace(" ","").replace("INDENT",";").replace("DEDENT",";")
        # print("code_1:\n{}".format(code_1))
        
        # obtain func2 names, strings stored in a list
        idx_ln_2 = idx_ln + 2   # the lines under the "def func_1_name()", which is idx_ln + 1
        while "</source>" not in lines[idx_ln_2]:
            if "def" in lines[idx_ln_2] \
            and ":" in lines[idx_ln_2] \
            and "default" not in lines[idx_ln_2] \
            and "@" not in lines[idx_ln_2]:
                ln_2 = lines[idx_ln_2]
                # print("ln_2:",ln_2)
                ln_2 = ln_2[ln_2.index("def ")+len("def "):]
                func_2_name = parse_function_name_line(ln_2)
                # obtain the startline and endline of func_2 by visiting the xml file again
                f_2 = open(function_xml_path, "r", encoding='utf-8')
                lines_2 = f_2.readlines()
                f_2.close()
                for idx_ln_3 in range(len(lines_2)):
                    if py_path in lines_2[idx_ln_3] and func_2_name in lines_2[idx_ln_3+1]:
                        # obtain startline and endline of func_2
                        ln_3 = lines_2[idx_ln_3]   # the line with <source file=
                        idx_st_2 = ln_3.index('startline="')
                        ln_3 = ln_3[idx_st_2+len('startline="'):]
                        start_line_2 = ln_3[:ln_3.index('"')]
                        # print("start_line_2",start_line_2)
                        idx_st_2 = ln_3.index('endline="')
                        ln_3 = ln_3[idx_st_2+len('endline="'):]
                        end_line_2 = ln_3[:ln_3.index('"')]
                        # print("end_line_2",end_line_2)
                        
                        # obtain code segment of func2
                        idx_ln_4 = idx_ln_3+2
                        code_2 = ""
                        while "</source>" not in lines_2[idx_ln_4]:
                            code_2 += lines_2[idx_ln_4]
                            idx_ln_4 += 1
                        code_2 = code_2.replace("\n","").replace(" ","").replace("INDENT",";").replace("DEDENT",";")
                        # print("code_2:\n{}".format(code_2))
                        # print(code_2 in code_1)
                        if code_2 in code_1 and int(start_line_2) >= int(start_line) and int(end_line_2) <= int(end_line):
                            function_in_xml[1].append(int(start_line_2))
                            function_in_xml[2].append(int(end_line_2))
                            function_in_xml[0].append(py_path)
                            function_in_xml[3].append(func_1_name)
                            function_in_xml[4].append(func_2_name)
                            # print("code_2 found for {}:\n{}".format(func_2_name,code_2))
                            # break
            idx_ln_2 += 1

print("{} py_path, {} start_line, {} end_line, {} func_1_name, {} func_2_name  found in the .xml file"
      .format(len(function_in_xml[0]),len(function_in_xml[1]),len(function_in_xml[2]),len(function_in_xml[3]),len(function_in_xml[4])))
print()
print("The first 5 records are:")
for i in range(5):
    print(function_in_xml[0][i], function_in_xml[1][i], function_in_xml[2][i], function_in_xml[3][i], function_in_xml[4][i])
print()

# check how many rows not assigned before processing
print(Counter(list(code_metric_function_2['function_location'])))
print()

# the format of the function name in the dataframe is py_file_name.func1_name.func2_name

for idx in range(len(function_in_xml[4])):
    py_path = function_in_xml[0][idx]
    py_file_name = py_path[get_last_index(py_path,'/')+1:py_path.index(".py")]
    func_1_name = function_in_xml[3][idx]
    # print(func_1_name)
    func_1_name = func_1_name[:func_1_name.index('(')] if '(' in func_1_name else func_1_name
    func_2_name = function_in_xml[4][idx].replace('(','\(').replace(')','\)').replace('*','\*')
    
    # print(repr(py_file_name), repr(func_1_name), repr(func_2_name))
    
    # look for the dataframe rows that contain both py_file_name and func_name
    dataset_tmp = code_metric_function_2[(code_metric_function_2['Name'].str.contains(py_file_name)) & 
                                         (code_metric_function_2['Name'].str.contains(func_1_name)) &
                                         (code_metric_function_2['Name'].str.contains(func_2_name))]
    # print(dataset_tmp.shape[0])
    if dataset_tmp.shape[0] == 1: 
        code_metric_function_2.loc[dataset_tmp.index,'function_location'] = function_in_xml[0][idx]
        code_metric_function_2.loc[dataset_tmp.index,'StartLine'] = function_in_xml[1][idx]
        code_metric_function_2.loc[dataset_tmp.index,'EndLine'] = function_in_xml[2][idx]
    elif dataset_tmp.shape[0] == 0:
        print("Not found, rows have {}, {}, and {}".format(py_file_name, function_in_xml[3][idx], function_in_xml[4][idx]))
    else:
        print("Multiple rows have {}, {}, and {}".format(py_file_name, function_in_xml[3][idx], function_in_xml[4][idx]))

# check how many rows not changed
print()
print(Counter(list(code_metric_function_2['function_location'])))


for idx in range(len(function_in_xml[4])):
    py_path = function_in_xml[0][idx]
    py_file_name = py_path[get_last_index(py_path,'/')+1:py_path.index(".py")]
    func_1_name = function_in_xml[3][idx]
    # print(func_1_name)
    func_1_name = func_1_name[:func_1_name.index('(')] if '(' in func_1_name else func_1_name
    func_2_name = function_in_xml[4][idx].replace('(','\(').replace(')','\)').replace('*','\*')
    
    # print(repr(py_file_name), repr(func_1_name), repr(func_2_name))
    
    # look for the dataframe rows that contain both py_file_name and func_name
    dataset_tmp = code_metric_function_2[(code_metric_function_2['Name'].str.contains(py_file_name)) & 
                                         (code_metric_function_2['Name']==func_1_name) &
                                         (code_metric_function_2['Name']==func_2_name) &
                                         (code_metric_function_2['function_location']=='not_assigned')]
    # print(dataset_tmp.shape[0])
    if dataset_tmp.shape[0] == 1: 
        code_metric_function_2.loc[dataset_tmp.index,'function_location'] = function_in_xml[0][idx]
        code_metric_function_2.loc[dataset_tmp.index,'StartLine'] = function_in_xml[1][idx]
        code_metric_function_2.loc[dataset_tmp.index,'EndLine'] = function_in_xml[2][idx]
    elif dataset_tmp.shape[0] == 0:
        print("Not found, rows have {}, {}, and {}".format(py_file_name, function_in_xml[3][idx], function_in_xml[4][idx]))
    else:
        print("Multiple rows have {}, {}, and {}".format(py_file_name, function_in_xml[3][idx], function_in_xml[4][idx]))

# check how many rows not changed
print()
print(Counter(list(code_metric_function_2['function_location'])))

code_metric_function_2.head()

# case 3 - use the py files and xml file to check the remaining functions

dataset_tmp = code_metric_function_2[code_metric_function_2['function_location']=='not_assigned']['Name']
func_to_check = list(dataset_tmp)
print("The functions that need to further check: {}\n{}".format(len(func_to_check),func_to_check[0:5]))
print()
func_to_check = [_[get_last_index(_,'.')+1:] for _ in func_to_check]
print("The names of the functions to check: {}\n{}".format(len(func_to_check),func_to_check[0:5]))
print()
func_in_xml_lines = {}
for idx_func in range(len(func_to_check)):
    if "," in func_to_check[idx_func]:
        _ = func_to_check[idx_func]
        _ = _.replace('(',',').replace(')',',')
        list_tmp = _.split(',')
        # print(list_tmp)
        func_in_xml_lines[func_to_check[idx_func]]=[]
        f = open(function_xml_path, "r", encoding='utf-8')
        lines = f.readlines()
        f.close()
        for idx_ln in range(len(lines)):
            if "def" in lines[idx_ln] and all_list_string_in_one_string_line(lines[idx_ln], list_tmp):
                # print("{} in line {}: {}".format(list_tmp, idx_ln, lines[idx_ln]))
                func_in_xml_lines[func_to_check[idx_func]].append(idx_ln)           
    else:
        # print(func_to_check[idx_func])
        func_in_xml_lines[func_to_check[idx_func]]=[]
        f = open(function_xml_path, "r", encoding='utf-8')
        lines = f.readlines()
        f.close()
        for idx_ln in range(len(lines)):
            if "def" in lines[idx_ln] and func_to_check[idx_func] in lines[idx_ln]:
                # print("{} in line {}: {}".format(func_to_check[idx_func], idx_ln, lines[idx_ln]))
                func_in_xml_lines[func_to_check[idx_func]].append(idx_ln)

function_to_verify_by_py_and_xml = []
for func in list(dataset_tmp):
    func_ = func[get_last_index(func,'.')+1:]
    if len(func_in_xml_lines[func_]) != 0:
        # print("{}:{}".format(func,func_in_xml_lines[func_]))
        function_to_verify_by_py_and_xml.append(func)
        # open the .py files in directories, obtain the startline and then compare to that in the .xml file
print("The functions that might be further parsed: {}\n{}".format(len(function_to_verify_by_py_and_xml), function_to_verify_by_py_and_xml[0:5]))
print()
        
# obtain all the .py files in the directory
# path_to_walk = "C:\\Users\\yoyo.yao\\Desktop\\Job_Lakehead University_Lab Technologist\\MSC_CS\\Lakehead_COMP5413_Special Topics on Software Engineering\\project4\\flask-main"
path_to_walk = MAIN_PATH + "code_metrics/py/flask-main"
path_file = []

for root, dirs, files in os.walk(path_to_walk, topdown=False):
    for _ in files:
        path_file.append(os.path.join(root, _))

py_file = [_ for _ in path_file if ".py" in _]
print("{} .py files in total".format(len(py_file)))
print("The first 5 .py files:\n{}".format(py_file[0:5]))

for func in function_to_verify_by_py_and_xml:
    # seprate the path, function name, and arguments
    # idx = get_last_index(func,".")
    # func_path = func[:idx].split(".")
    str1 = func
    idx = get_last_index(str1,".")
    str1 = str1[:idx]
    str1 = str1[:get_first_uppercase_index(str1)] if get_first_uppercase_index(str1) != None else str1
    func_path = str1.replace("."," ").split()    
    # print(str1)        
    func_name = func[idx+1:][:func[idx+1:].index('(')]
    func_name = "def " + func_name 
    func_argu = func[idx+1:][func[idx+1:].index('(')+1:].replace(',',' ').replace(')',' ').split()
    
    for py in py_file:
        # if the func path seems to match the py file path, open this py file and check
        if all_list_string_in_one_string_line(py, func_path):
            f_py = open(py, "r", encoding='utf-8')
            lines_py = f_py.readlines()
            f_py.close()
            # if the function name and arguments are all in the line, retrieve the line number and read xml file to compare
            # if the startline number matches and only 1 match is found, we assume the function is found
            for idx_ln_py in range(len(lines_py)):
                if not all_list_string_in_one_string_line(lines_py[idx_ln_py], ["def","(",")",":"]):
                    continue
                if any_list_string_in_one_string_line(lines_py[idx_ln_py], ["-","{","}","@","default"]):
                    continue
                # print(lines_py[idx_ln_py])
                if func_name != lines_py[idx_ln_py][lines_py[idx_ln_py].index("def"):lines_py[idx_ln_py].index("(")]:
                    continue
                if len(func_argu) == 0 or (len(func_argu) != 0 and all_list_string_in_one_string_line(lines_py[idx_ln_py], func_argu)):
                    # retrieve the start line number in .py file
                    start_line_py = idx_ln_py + 1
                    # check the information in the xml file
                    f_xml = open(function_xml_path, "r", encoding='utf-8')
                    lines_xml = f_xml.readlines()
                    f_xml.close()
                    function_match_found = []
                    for idx_ln_xml in range(len(lines_xml)):
                        # if the function name and arguments are all in the line of xml file, retrieve the line number
                        if not all_list_string_in_one_string_line(lines_xml[idx_ln_xml], ["def","(",")",":"]):
                            continue
                        if any_list_string_in_one_string_line(lines_xml[idx_ln_xml], ["-","{","}","@","default"]):
                            continue
                        # print(lines_xml[idx_ln_xml])
                        if func_name != lines_xml[idx_ln_xml][lines_xml[idx_ln_xml].index("def"):lines_xml[idx_ln_xml].index("(")]:
                            continue
                        if (len(func_argu) == 0 or (len(func_argu) != 0 \
                            and all_list_string_in_one_string_line(lines_xml[idx_ln_xml], func_argu))) \
                            and "<source file=" in lines_xml[idx_ln_xml-1]:
                            ln_xml_func = lines_xml[idx_ln_xml-1]
                            # print(ln_xml_func)
                            ln_xml_func = ln_xml_func[ln_xml_func.index('startline="')+len('startline="'):]
                            start_line_xml = int(ln_xml_func[:ln_xml_func.index('"')])
                            if start_line_xml == start_line_py:
                                ln_xml_func = lines_xml[idx_ln_xml-1]
                                ln_xml_func = ln_xml_func[ln_xml_func.index('flask-main'):]
                                func_path_xml = ln_xml_func[:ln_xml_func.index('"')]
                                ln_xml_func = lines_xml[idx_ln_xml-1]
                                ln_xml_func = ln_xml_func[ln_xml_func.index('endline="')+len('endline="'):]
                                end_line_xml = int(ln_xml_func[:ln_xml_func.index('"')])
                                function_match_found.append([func_path_xml, start_line_xml, end_line_xml])
                    # after traverse each line in the xml file, decide whether the result is unique
                    # if yes, assign it to the dataframe
                    if len(function_match_found)==1:
                        print("Match found for {} at line {} in {} and {}".format(func, start_line_py, py, lines_xml[idx_ln_xml-1]))
                        # assign values to dataframe
                        # look for the dataframe rows that contains the specific function name and has not been assigned
                        dataset_tmp = code_metric_function_2[(code_metric_function_2['Name']==func) &
                                                             (code_metric_function_2['function_location']=='not_assigned')]
                        if dataset_tmp.shape[0] == 1: 
                            code_metric_function_2.loc[dataset_tmp.index,'function_location'] = function_match_found[0][0]
                            code_metric_function_2.loc[dataset_tmp.index,'StartLine'] = function_match_found[0][1]
                            code_metric_function_2.loc[dataset_tmp.index,'EndLine'] = function_match_found[0][2]
                        elif dataset_tmp.shape[0] == 0:
                            print("Not found, rows have {}".format(func))
                        else:
                            print("Multiple rows have {}".format(func))    
                            
                    elif len(function_match_found)>1:
                        print("Multiple matches found for {} at {}".format(func, function_match_found))
                    else:
                        print("No matches found for {}".format(func))
                            
# check how many rows not changed
print()
print(Counter(list(code_metric_function_2['function_location'])))

code_metric_function_2.head()

"""### 2.1.5 Drop the instances with 'function_location' being 'not_assigned'"""

flask_dataset = code_metric_function_2[code_metric_function_2['function_location']!='not_assigned']

print("There are {} instances.".format(flask_dataset.shape[0]))

flask_dataset.head()

"""### 2.1.6 Add the label column 'is_cloned' for labelling based on XML from NiCad"""

flask_dataset['is_cloned']=[0 for _ in range(flask_dataset.shape[0])]
flask_dataset = flask_dataset[['function_location',
                                                 'Name',
                                                 'StartLine',
                                                 'EndLine',
                                                 'CountLine', 
                                                 'CountLineBlank', 
                                                 'CountLineCode', 
                                                 'CountLineCodeDecl', 
                                                 'CountLineCodeExe', 
                                                 'CountLineComment', 
                                                 'CountPath', 
                                                 'CountPathLog', 
                                                 'CountStmt', 
                                                 'CountStmtDecl', 
                                                 'CountStmtExe', 
                                                 'Cyclomatic', 
                                                 'CyclomaticModified', 
                                                 'CyclomaticStrict', 
                                                 'Essential', 
                                                 'MaxNesting', 
                                                 'RatioCommentToCode',
                                                 'is_cloned']]
flask_dataset.head()

"""### 2.1.7 Parse the NiCad function clone XML file to label the 'is_cloned' column"""

# parse the functions.xml file to locate the functions
# function_clone_xml_path = ["C:\\Users\\yoyo.yao\\Desktop\\Job_Lakehead University_Lab Technologist\\MSC_CS\\Lakehead_COMP5413_Special Topics on Software Engineering\\project4\\flask\\type1_flask-main_functions-clones\\flask-main_functions-clones-0.00.xml",
#                            "C:\\Users\\yoyo.yao\\Desktop\\Job_Lakehead University_Lab Technologist\\MSC_CS\\Lakehead_COMP5413_Special Topics on Software Engineering\\project4\\flask\\type2_flask-main_functions-blind-clones\\flask-main_functions-blind-clones-0.00.xml",
#                            "C:\\Users\\yoyo.yao\\Desktop\\Job_Lakehead University_Lab Technologist\\MSC_CS\\Lakehead_COMP5413_Special Topics on Software Engineering\\project4\\flask\\type2c_flask-main_functions-consistent-clones\\flask-main_functions-consistent-clones-0.00.xml",
#                            "C:\\Users\\yoyo.yao\\Desktop\\Job_Lakehead University_Lab Technologist\\MSC_CS\\Lakehead_COMP5413_Special Topics on Software Engineering\\project4\\flask\\type3-1_flask-main_functions-clones\\flask-main_functions-clones-0.30.xml",
#                            "C:\\Users\\yoyo.yao\\Desktop\\Job_Lakehead University_Lab Technologist\\MSC_CS\\Lakehead_COMP5413_Special Topics on Software Engineering\\project4\\flask\\type3-2_flask-main_functions-blind-clones\\flask-main_functions-blind-clones-0.30.xml",
#                            "C:\\Users\\yoyo.yao\\Desktop\\Job_Lakehead University_Lab Technologist\\MSC_CS\\Lakehead_COMP5413_Special Topics on Software Engineering\\project4\\flask\\type3-2c_flask-main_functions-consistent-clones\\flask-main_functions-consistent-clones-0.30.xml"]

function_clone_xml_path = [MAIN_PATH + "code_metrics/xml/flask/type1_flask-main_functions-clones/flask-main_functions-clones-0.00.xml",
                           MAIN_PATH + "code_metrics/xml/flask/type2_flask-main_functions-blind-clones/flask-main_functions-blind-clones-0.00.xml",
                           MAIN_PATH + "code_metrics/xml/flask/type2c_flask-main_functions-consistent-clones/flask-main_functions-consistent-clones-0.00.xml",
                           MAIN_PATH + "code_metrics/xml/flask/type3-1_flask-main_functions-clones/flask-main_functions-clones-0.30.xml",
                           MAIN_PATH + "code_metrics/xml/flask/type3-2_flask-main_functions-blind-clones/flask-main_functions-blind-clones-0.30.xml",
                           MAIN_PATH + "code_metrics/xml/flask/type3-2c_flask-main_functions-consistent-clones/flask-main_functions-consistent-clones-0.30.xml"]

for xml_path in function_clone_xml_path:
    # read lines of each xml file
    f = open(xml_path, "r", encoding='utf-8')
    lines = f.readlines()
    f.close()
    
    # parse the line and assign the label to corresponding row
    for idx in range(len(lines)):
        ln = lines[idx]
        if "<source file=" in ln:
            ln = ln[ln.index("flask-main"):]
            func_loc = ln[:ln.index('"')]
            ln = ln[ln.index('startline="')+len('startline="'):]
            start_line = int(ln[:ln.index('"')])
            ln = ln[ln.index('endline="')+len('endline="'):]
            end_line = int(ln[:ln.index('"')])
            # print(func_loc, start_line, end_line)
            
            # update the dataframe
            # look for the dataframe rows that contain both py_file_name and func_name
            dataset_tmp = flask_dataset[(flask_dataset['function_location']==func_loc) &
                                        (flask_dataset['StartLine']==start_line) &
                                        (flask_dataset['EndLine']==end_line) &
                                        (flask_dataset['is_cloned']==0)]
            
            if dataset_tmp.shape[0] == 1:
                flask_dataset.loc[dataset_tmp.index,'is_cloned'] = 1
    print("Parsing cloned functions is completed for {}.".format(xml_path))

# check how many rows not changed
print()
print(Counter(list(flask_dataset['is_cloned'])))

flask_dataset.head()

"""## 2.2 Split and save Train and Test datasets"""

dataset_1 = flask_dataset[['CountLine', 
                            'CountLineBlank', 
                            'CountLineCode', 
                            'CountLineCodeDecl', 
                            'CountLineCodeExe', 
                            'CountLineComment', 
                            'CountPath', 
                            'CountPathLog', 
                            'CountStmt', 
                            'CountStmtDecl', 
                            'CountStmtExe', 
                            'Cyclomatic', 
                            'CyclomaticModified', 
                            'CyclomaticStrict', 
                            'Essential', 
                            'MaxNesting', 
                            'RatioCommentToCode',
                            'is_cloned']]
dataset_1.head()

# count the values occurrence in each column
for _ in dataset_1.columns.tolist():
  print(_)
  print(Counter(dataset_1[_]))

# save and load dataframe
dataframe_flask_path = "/content/drive/MyDrive/Software_data/code_metrics/dataframe_flask.pkl"

dataset_1.to_pickle(dataframe_flask_path) 

dataset_1 = pd.read_pickle(dataframe_flask_path)

dataset_1 = shuffle(dataset_1)
dataset_1_X = dataset_1.iloc[:,0:dataset_1.shape[1]-1]
print("dataset_1_X has a shape of {}".format(dataset_1_X.shape))
print("the first 5 instances:\n{}".format(dataset_1_X.iloc[0:5,:]))
dataset_1_y = dataset_1.iloc[:,-1:]
print("dataset_1_y has a shape of {}".format(dataset_1_y.shape))
print("the first 5 instances:\n{}".format(dataset_1_y.iloc[0:5,:]))
print()

# split into train test sets
dataset_1_X_train, dataset_1_X_test, dataset_1_y_train, dataset_1_y_test = train_test_split(dataset_1_X, dataset_1_y, test_size=0.2, stratify = dataset_1_y)
np.save(MAIN_PATH+"dataset/flask_X_train.npy",np.array(dataset_1_X_train))
np.save(MAIN_PATH+"dataset/flask_X_test.npy",np.array(dataset_1_X_test))
np.save(MAIN_PATH+"dataset/flask_y_train.npy",np.array(dataset_1_y_train).flatten())
np.save(MAIN_PATH+"dataset/flask_y_test.npy",np.array(dataset_1_y_test).flatten())
# dataset_1_X_train_back = dataset_1_X_train.copy()
# dataset_1_X_test_back = dataset_1_X_test.copy()

# verify the class distribution
print("class 0:1 in dataset_1_y is       : {}:{}".format(Counter(list(dataset_1_y['is_cloned']))[0]/dataset_1_y.shape[0], 1-Counter(list(dataset_1_y['is_cloned']))[0]/dataset_1_y.shape[0]))
print("class 0:1 in dataset_1_y_train is : {}:{}".format(Counter(list(dataset_1_y_train['is_cloned']))[0]/dataset_1_y_train.shape[0], 1-Counter(list(dataset_1_y_train['is_cloned']))[0]/dataset_1_y_train.shape[0]))
print("class 0:1 in dataset_1_y_test is  : {}:{}".format(Counter(list(dataset_1_y_test['is_cloned']))[0]/dataset_1_y_test.shape[0], 1-Counter(list(dataset_1_y_test['is_cloned']))[0]/dataset_1_y_test.shape[0]))
print()

# the train set will be further split by stratified splitting for cross validation
skf = StratifiedKFold(n_splits=5)
skf_tr_va_sets = skf.split(dataset_1_X_train, dataset_1_y_train)
for idx_f, (idx_tr, idx_va) in enumerate(skf_tr_va_sets):
  tr_X = dataset_1_X.iloc[idx_tr]
  tr_y = dataset_1_y.iloc[idx_tr]
  va_X = dataset_1_X.iloc[idx_va]
  va_y = dataset_1_y.iloc[idx_va]
  print("Fold {}\ntrain set X:{} train set y: {} validation set X:{} validation set y: {}".format(idx_f,tr_X.shape,tr_y.shape,va_X.shape,va_y.shape))
  print("class 0:1 in train set y is      : {}:{}".format(Counter(list(tr_y['is_cloned']))[0]/tr_y.shape[0], 1-Counter(list(tr_y['is_cloned']))[0]/tr_y.shape[0]))
  print("class 0:1 in validation set y is : {}:{}".format(Counter(list(va_y['is_cloned']))[0]/va_y.shape[0], 1-Counter(list(va_y['is_cloned']))[0]/va_y.shape[0]))

"""## 2.3 Train and Test - Original Dataset

### 2.3.1 DT
"""

save_path = MAIN_PATH + "model/"
data_type = "flask"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='dt')

"""### 2.3.2 RF"""

save_path = MAIN_PATH + "model/"
data_type = "flask"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='rf')

"""### 2.3.3 KNN"""

save_path = MAIN_PATH + "model/"
data_type = "flask"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='knn')

"""### 2.3.4 NB"""

save_path = MAIN_PATH + "model/"
data_type = "flask"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nb')

"""### 2.3.5 LR"""

save_path = MAIN_PATH + "model/"
data_type = "flask"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lr')

"""### 2.3.6 LDA"""

save_path = MAIN_PATH + "model/"
data_type = "flask"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lda')

"""### 2.3.7 QDA"""

save_path = MAIN_PATH + "model/"
data_type = "flask"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='qda')

"""### 2.3.8 SVM"""

save_path = MAIN_PATH + "model/"
data_type = "flask"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='svm')

"""### 2.3.9 MLP"""

save_path = MAIN_PATH + "model/"
data_type = "flask"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='mlp')

"""### 2.3.10 NN"""

save_path = MAIN_PATH + "model/"
data_type = "flask"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nn')

"""## 2.4 Feature Selection

by Accuracy
"""

# use RF to select features
data_type = "flask"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")

model = RandomForestClassifier(max_depth=None, random_state=0)
feature_percentage = 1.0

# Sequential Forward Selection by accuracy
sfs_acc = SFS(model, 
          k_features=int(X_train.shape[1]*feature_percentage),   # select 80% feature columns
          forward=True, 
          floating=False, 
          verbose=2,
          scoring='accuracy',
          cv=5,                                 # cross validation
          n_jobs=-1)                            # run CV on all CPU cores

sfs_acc = sfs_acc.fit(X_train, y_train)

"""by F1-score"""

# use RF to select features
data_type = "flask"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")

model = RandomForestClassifier(max_depth=None, random_state=0)
feature_percentage = 1.0

# Sequential Forward Selection by F1-score
sfs_f1 = SFS(model, 
          k_features=int(X_train.shape[1]*feature_percentage),   # select 80% feature columns
          forward=True, 
          floating=False, 
          verbose=2,
          scoring='f1',
          cv=5,                                 # cross validation
          n_jobs=-1)                            # run CV on all CPU cores

sfs_f1 = sfs_f1.fit(X_train, y_train)

# select feature that increase average cv score by accuracy
print('Sequential Forward Selection (k={}) by Accuracy'.format(int(X_train.shape[1]*feature_percentage)))
print("Indices of selected features: {}".format(sfs_acc.k_feature_idx_))
print('CV Score                    : {}'.format(sfs_acc.k_score_))

data_type = "flask"
feature_name = pd.read_pickle(MAIN_PATH + "code_metrics/dataframe_"+data_type+".pkl").columns.tolist()
feature_name = feature_name[:-1]

feature_name_selected = [feature_name[idx] for idx in sfs_acc.k_feature_idx_]
print("Selected features           : {}".format(feature_name_selected))

feature_idx_selected_by_sequence = []
feature_cv_avg_score = []
for _ in sfs_acc.subsets_:
  # print(sfs_acc.subsets_[_])
  for idx in sfs_acc.subsets_[_]['feature_names']:
    if int(idx) not in feature_idx_selected_by_sequence:
      feature_idx_selected_by_sequence.append(int(idx))
      feature_cv_avg_score.append(sfs_acc.subsets_[_]['avg_score'])

print("Feature indices selected by sequence               : {}".format(feature_idx_selected_by_sequence))
print("CV average score by current feature selection      : {}".format(feature_cv_avg_score))

# select only the features keep increasing the CV average score
target_idx = 0
for idx in range(len(feature_cv_avg_score)):
  if idx+1 == len(feature_cv_avg_score) or feature_cv_avg_score[idx+1] <= feature_cv_avg_score[idx]:
    target_idx = idx
    break

feature_idx_increase_cv_score =  feature_idx_selected_by_sequence[:idx+1]
print("The feature indices that increase CV average score : {}".format(feature_idx_increase_cv_score))
feature_name_increase_cv_score = [feature_name[idx] for idx in feature_idx_increase_cv_score]
print("The feature names that increase CV average score   : {}".format(feature_name_increase_cv_score))
feature_idx_selected_by_acc = feature_idx_increase_cv_score

# select feature that increase average cv score by f1-score
print()
print('Sequential Forward Selection (k={}) by F1-score'.format(int(X_train.shape[1]*feature_percentage)))
print("Indices of selected features: {}".format(sfs_f1.k_feature_idx_))
print('CV Score                    : {}'.format(sfs_f1.k_score_))

data_type = "flask"
feature_name = pd.read_pickle(MAIN_PATH + "code_metrics/dataframe_"+data_type+".pkl").columns.tolist()
feature_name = feature_name[:-1]

feature_name_selected = [feature_name[idx] for idx in sfs_f1.k_feature_idx_]
print("Selected features           : {}".format(feature_name_selected))

feature_idx_selected_by_sequence = []
feature_cv_avg_score = []
for _ in sfs_f1.subsets_:
  # print(sfs_f1.subsets_[_])
  for idx in sfs_f1.subsets_[_]['feature_names']:
    if int(idx) not in feature_idx_selected_by_sequence:
      feature_idx_selected_by_sequence.append(int(idx))
      feature_cv_avg_score.append(sfs_f1.subsets_[_]['avg_score'])

print("Feature indices selected by sequence               : {}".format(feature_idx_selected_by_sequence))
print("CV average score by current feature selection      : {}".format(feature_cv_avg_score))

# select only the features keep increasing the CV average score
target_idx = 0
for idx in range(len(feature_cv_avg_score)):
  if idx+1 == len(feature_cv_avg_score) or feature_cv_avg_score[idx+1] <= feature_cv_avg_score[idx]:
    target_idx = idx
    break

feature_idx_increase_cv_score =  feature_idx_selected_by_sequence[:idx+1]
print("The feature indices that increase CV average score : {}".format(feature_idx_increase_cv_score))
feature_name_increase_cv_score = [feature_name[idx] for idx in feature_idx_increase_cv_score]
print("The feature names that increase CV average score   : {}".format(feature_name_increase_cv_score))
feature_idx_selected_by_f1 = feature_idx_increase_cv_score

# keep the intersection of two lists
feature_idx_increase_cv_score = list(set(feature_idx_selected_by_acc) & set(feature_idx_selected_by_f1))

# build the feature set using the selected features
data_type = "flask"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

data_type = "flask_sfs"
np.save(MAIN_PATH+"dataset/"+data_type+"_X_train.npy",X_train[:,feature_idx_increase_cv_score])
np.save(MAIN_PATH+"dataset/"+data_type+"_y_train.npy",y_train)
np.save(MAIN_PATH+"dataset/"+data_type+"_X_test.npy",X_test[:,feature_idx_increase_cv_score])
np.save(MAIN_PATH+"dataset/"+data_type+"_y_test.npy",y_test)

"""## 2.5 Train and Test - SFS Dataset

### 2.5.1 DT
"""

save_path = MAIN_PATH + "model/"
data_type = "flask_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='dt')

"""### 2.5.2 RF"""

save_path = MAIN_PATH + "model/"
data_type = "flask_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='rf')

"""### 2.5.3 KNN"""

save_path = MAIN_PATH + "model/"
data_type = "flask_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='knn')

"""### 2.5.4 NB"""

save_path = MAIN_PATH + "model/"
data_type = "flask_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nb')

"""### 2.5.5 LR"""

save_path = MAIN_PATH + "model/"
data_type = "flask_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lr')

"""### 2.5.6 LDA"""

save_path = MAIN_PATH + "model/"
data_type = "flask_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lda')

"""### 2.5.7 QDA"""

save_path = MAIN_PATH + "model/"
data_type = "flask_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='qda')

"""### 2.5.8 SVM"""

save_path = MAIN_PATH + "model/"
data_type = "flask_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='svm')

"""### 2.5.9 MLP"""

save_path = MAIN_PATH + "model/"
data_type = "flask_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='mlp')

"""### 2.5.10 NN"""

save_path = MAIN_PATH + "model/"
data_type = "flask_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nn')

"""## 2.6 Ensemble"""

save_path = MAIN_PATH + "model/"
data_type = "flask"
metrics = os.listdir(save_path)

# select classifiers by thresholds
th_acc = 0.7
th_f1s = 0.7
clf_sel = []

print("Results:")
print("-"*80)
metrics_title = ['Accuracy','F1-score','Precision','Recall','Confusion Matrix']
print(" "*45,end="")
for m_t in metrics_title:
  print("{: >11} ".format(m_t),end="")
print()
# list label encoding classifiers
idx_clf = 0
for me in metrics:
  if "metric" in me:
    if data_type in me:    # model_le_smote_svm.pkl, true_le_smote_svm.npy, pred_le_smote_svm.npy, metric_le_smote_svm.npy
      idx_clf += 1
      clf_name = me[len("metric_"):len(me)-len(".npy")] 
      print("Classifier #{:2d}: {:20s} ".format(idx_clf, clf_name), end="")
      me_list = np.load(save_path+me)
      me_list = me_list.tolist()
      print("Metrics:",end="")
      for m_l in me_list:
        print("{: >11} ".format("{:4.7f}".format(m_l)),end="")
      print()

      # select the classifier if it meets thresholds
      if me_list[0]>=th_acc and me_list[1]>=th_f1s:
        clf_sel.append(clf_name)

print("{} classifiers selected:{}\n".format(len(clf_sel),clf_sel))
if len(clf_sel) > 0:
  # output the true and predictions of each selected classifier
  for i in range(len(clf_sel)):
    print("classifier #{:2d}".format(i+1))
    print("True      :",np.load(save_path+"true_"+clf_sel[i]+".npy").astype(np.uint8).tolist())
    print("Prediction:",np.load(save_path+"pred_"+clf_sel[i]+".npy").astype(np.uint8).tolist())

  # check whether the saved true values are the same as the original values
  flask_y_test = np.load(MAIN_PATH+"dataset/"+"flask_y_test.npy")

  # check whether each saved y values match the label encoding y values
  for i in range(len(clf_sel)):
    true = np.load(save_path+"true_"+clf_sel[i]+".npy")
    for j in range(len(true)):
      if list(flask_y_test)[j] - true.flatten()[j] !=0:
        print("y values of label encoding does not match those of saved y values for {}".format(clf_sel[i]))
        break
      elif j == len(true)-1:
        print("y values of label encoding match those of saved y vaues for {}".format(clf_sel[i]))  
      
  # generate the prediction values by voting
  pred = []
  for i in range(len(clf_sel)):
    pred.append(np.load(save_path+"pred_"+clf_sel[i]+".npy").astype(np.uint8).tolist())
  pred = np.array(pred)
  pred_ensemble = []
  for i in range(pred.shape[1]):
    if sum(pred[:,i])>=pred.shape[0]-sum(pred[:,i]):
      pred_ensemble.append(1)
    else:
      pred_ensemble.append(0)
  true = np.load(save_path+"true_"+clf_sel[0]+".npy").astype(np.uint8).tolist()
  print("True                :",true)
  print("Prediction by voting:",pred_ensemble)

  print("Accuracy score             :",accuracy_score(true, pred_ensemble))
  print("F1-score                   :",f1_score(true, pred_ensemble))
  print("Precision score            :",precision_score(true, pred_ensemble))
  print("Recall score               :",recall_score(true, pred_ensemble))
  print("Classification report      :\n",classification_report(true, pred_ensemble))
  print("Confusion Matrix           :\n",confusion_matrix(true, pred_ensemble))

  # plot confusion matrix
  cm = confusion_matrix(true, pred_ensemble)
  plot_confusion_matrix(cm, normalize = True, target_names = ["not readmitted","readmitted"], title = "Confusion Matrix")

else:
  print("No reliable results!")

"""# 3\. Tornado

## 3.1 Extracting data - the same procedures as that of Flask, not to show details
"""

# code_metric = "C:\\Users\\yoyo.yao\\Desktop\\Job_Lakehead University_Lab Technologist\\MSC_CS\\Lakehead_COMP5413_Special Topics on Software Engineering\\project4\\tornado-master.csv"
code_metric = MAIN_PATH + "code_metrics/csv/tornado-master.csv"
code_metric_dataset = pd.read_csv(code_metric)
# code_metric_dataset.head()

code_metric_function = code_metric_dataset[code_metric_dataset['Kind']=='Function']
# code_metric_function.head()

code_metric_function = code_metric_function[['Name', 
                                             'CountLine', 
                                             'CountLineBlank', 
                                             'CountLineCode', 
                                             'CountLineCodeDecl', 
                                             'CountLineCodeExe', 
                                             'CountLineComment', 
                                             'CountPath', 
                                             'CountPathLog', 
                                             'CountStmt', 
                                             'CountStmtDecl', 
                                             'CountStmtExe', 
                                             'Cyclomatic', 
                                             'CyclomaticModified', 
                                             'CyclomaticStrict', 
                                             'Essential', 
                                             'MaxNesting', 
                                             'RatioCommentToCode']]
# code_metric_function.head()

code_metric_function_2 = code_metric_function
code_metric_function_2['function_location']=['not_assigned' for _ in range(code_metric_function.shape[0])]
code_metric_function_2['StartLine']=[-1 for _ in range(code_metric_function.shape[0])]
code_metric_function_2['EndLine']=[-1 for _ in range(code_metric_function.shape[0])]
code_metric_function_2 = code_metric_function_2[['function_location',
                                                 'Name',
                                                 'StartLine',
                                                 'EndLine',
                                                 'CountLine', 
                                                 'CountLineBlank', 
                                                 'CountLineCode', 
                                                 'CountLineCodeDecl', 
                                                 'CountLineCodeExe', 
                                                 'CountLineComment', 
                                                 'CountPath', 
                                                 'CountPathLog', 
                                                 'CountStmt', 
                                                 'CountStmtDecl', 
                                                 'CountStmtExe', 
                                                 'Cyclomatic', 
                                                 'CyclomaticModified', 
                                                 'CyclomaticStrict', 
                                                 'Essential', 
                                                 'MaxNesting', 
                                                 'RatioCommentToCode']]
code_metric_function_2.head()

# look for the function names in the dataframe, if there is no duplicated name, assign the values to the dataframe row

# case 1 - the function in the dataframe has no replicated records, totally matched by function names
# the format of the function name in the dataframe is py_file_name.x.func_name

# parse the functions.xml file to locate the functions
# function_xml_path = "C:\\Users\\yoyo.yao\\Desktop\\Job_Lakehead University_Lab Technologist\\MSC_CS\\Lakehead_COMP5413_Special Topics on Software Engineering\\project4\\tornado\\type1_tornado-master_functions.xml"
function_xml_path = MAIN_PATH + "code_metrics/xml/tornado/type1_tornado-master_functions.xml"
f = open(function_xml_path, "r", encoding='utf-8')
lines = f.readlines()
f.close()

function_in_xml = [[], # store the py file location
                   [], # start line
                   [], # end line
                   []] # function name after processing

for idx_ln in range(len(lines)):
    ln = lines[idx_ln]
    if "<source file=" in ln:
        idx_st = ln.index("tornado-master")
        idx_ed = ln.index(".py")
        py_path = ln[idx_st:idx_ed+len(".py")]
        # print(py_path)
        idx_st = ln.index('startline="')
        ln = ln[idx_st+len('startline="'):]
        start_line = ln[:ln.index('"')]
        # print(start_line)
        idx_st = ln.index('endline="')
        ln = ln[idx_st+len('endline="'):]
        end_line = ln[:ln.index('"')]
        # print(end_line)
        ln = lines[idx_ln+1]
        ln = ln[ln.index("def ")+len("def "):]
        func_name = parse_function_name_line(ln)
        # print(func_name)
        function_in_xml[0].append(py_path)
        function_in_xml[1].append(int(start_line))
        function_in_xml[2].append(int(end_line))
        function_in_xml[3].append(func_name)    
        
print("{} functions found in the .xml file".format(len(function_in_xml[0])))
print()
print("The first 5 records are:")
for i in range(5):
    print(function_in_xml[0][i], function_in_xml[1][i], function_in_xml[2][i], function_in_xml[3][i])
print()
 
# check how many rows not assigned before processing
print(Counter(list(code_metric_function_2['function_location'])))
print()

for idx in range(len(function_in_xml[3])):
    py_path = function_in_xml[0][idx]
    py_file_name = py_path[get_last_index(py_path,'/')+1:py_path.index(".py")]
    func_name = function_in_xml[3][idx].replace('(','\(').replace(')','\)').replace('*','\*')
    
    # print(repr(py_file_name), repr(func_name))
    
    # look for the dataframe rows that contain both py_file_name and func_name
    dataset_tmp = code_metric_function_2[(code_metric_function_2['Name'].str.contains(py_file_name)) & 
                                         (code_metric_function_2['Name'].str.contains(func_name)) &
                                         (code_metric_function_2['function_location']=='not_assigned')]
    # print(dataset_tmp)
    if dataset_tmp.shape[0] == 1: 
        code_metric_function_2.loc[dataset_tmp.index,'function_location'] = function_in_xml[0][idx]
        code_metric_function_2.loc[dataset_tmp.index,'StartLine'] = function_in_xml[1][idx]
        code_metric_function_2.loc[dataset_tmp.index,'EndLine'] = function_in_xml[2][idx]
    elif dataset_tmp.shape[0] == 0:
        print("Not found, rows have {} and {}".format(py_file_name, function_in_xml[3][idx]))
    else:
        print("Multiple rows have {} and {}".format(py_file_name, function_in_xml[3][idx]))

# check how many rows not changed
print()
print(Counter(list(code_metric_function_2['function_location'])))

for idx in range(len(function_in_xml[3])):
    py_path = function_in_xml[0][idx]
    py_file_name = py_path[get_last_index(py_path,'/')+1:py_path.index(".py")]
    func_name = function_in_xml[3][idx].replace('(','\(').replace(')','\)').replace('*','\*')
    
    # print(repr(py_file_name), repr(func_name))
    
    # look for the dataframe rows that contain both py_file_name and func_name
    dataset_tmp = code_metric_function_2[(code_metric_function_2['Name'].str.contains(py_file_name)) & 
                                         (code_metric_function_2['Name']==func_name) &
                                         (code_metric_function_2['function_location']=='not_assigned')]
    # print(dataset_tmp)
    if dataset_tmp.shape[0] == 1: 
        code_metric_function_2.loc[dataset_tmp.index,'function_location'] = function_in_xml[0][idx]
        code_metric_function_2.loc[dataset_tmp.index,'StartLine'] = function_in_xml[1][idx]
        code_metric_function_2.loc[dataset_tmp.index,'EndLine'] = function_in_xml[2][idx]
    elif dataset_tmp.shape[0] == 0:
        print("Not found, rows have {} and {}".format(py_file_name, function_in_xml[3][idx]))
    else:
        print("Multiple rows have {} and {}".format(py_file_name, function_in_xml[3][idx]))

# check how many rows not changed
print()
print(Counter(list(code_metric_function_2['function_location'])))

code_metric_function_2.head()

# case 2 - the function is nested in another function
# the format of the function name in the dataframe is py_file_name.func1_name.func2_name

# parse the functions.xml file to locate the functions
# function_xml_path = "C:\\Users\\yoyo.yao\\Desktop\\Job_Lakehead University_Lab Technologist\\MSC_CS\\Lakehead_COMP5413_Special Topics on Software Engineering\\project4\\tornado\\type1_tornado-master_functions.xml"
function_xml_path = MAIN_PATH + "code_metrics/xml/tornado/type1_tornado-master_functions.xml"
f = open(function_xml_path, "r", encoding='utf-8')
lines = f.readlines()
f.close()

function_in_xml = [[], # store the py file location
                   [], # start line of func2
                   [], # end line of func2
                   [], # func1 name after processing
                   []] # func2 name after processing

for idx_ln in range(len(lines)):
    ln = lines[idx_ln]
    if "<source file=" in ln:
        idx_st = ln.index("tornado-master")
        idx_ed = ln.index(".py")
        py_path = ln[idx_st:idx_ed+len(".py")]
        # print(py_path)
        idx_st = ln.index('startline="')
        ln = ln[idx_st+len('startline="'):]
        start_line = ln[:ln.index('"')]
        # print("start_line_1",start_line)
        idx_st = ln.index('endline="')
        ln = ln[idx_st+len('endline="'):]
        end_line = ln[:ln.index('"')]
        # print("end_line_1",end_line)
        ln = lines[idx_ln+1]
        ln = ln[ln.index("def ")+len("def "):]
        func_1_name = parse_function_name_line(ln)
        # print(func_name)
        
        # obtain code segment of func1
        idx_ln_1 = idx_ln + 1
        code_1 = ""
        while "</source>" not in lines[idx_ln_1]: 
            code_1 += lines[idx_ln_1]
            idx_ln_1 += 1
        code_1 = code_1.replace("\n","").replace(" ","").replace("INDENT",";").replace("DEDENT",";")
        # print("code_1:\n{}".format(code_1))
        
        # obtain func2 names, strings stored in a list
        idx_ln_2 = idx_ln + 2   # the lines under the "def func_1_name()", which is idx_ln + 1
        while "</source>" not in lines[idx_ln_2]:
            if "def" in lines[idx_ln_2] \
            and ":" in lines[idx_ln_2] \
            and "default" not in lines[idx_ln_2] \
            and "define" not in lines_2[idx_ln_2] \
            and "@" not in lines[idx_ln_2]:
                ln_2 = lines[idx_ln_2]
                # print("ln_2:",ln_2)
                ln_2 = ln_2[ln_2.index("def ")+len("def "):]
                func_2_name = parse_function_name_line(ln_2)
                # obtain the startline and endline of func_2 by visiting the xml file again
                f_2 = open(function_xml_path, "r", encoding='utf-8')
                lines_2 = f_2.readlines()
                f_2.close()
                for idx_ln_3 in range(len(lines_2)):
                    if py_path in lines_2[idx_ln_3] and func_2_name in lines_2[idx_ln_3+1]:
                        # obtain startline and endline of func_2
                        ln_3 = lines_2[idx_ln_3]   # the line with <source file=
                        idx_st_2 = ln_3.index('startline="')
                        ln_3 = ln_3[idx_st_2+len('startline="'):]
                        start_line_2 = ln_3[:ln_3.index('"')]
                        # print("start_line_2",start_line_2)
                        idx_st_2 = ln_3.index('endline="')
                        ln_3 = ln_3[idx_st_2+len('endline="'):]
                        end_line_2 = ln_3[:ln_3.index('"')]
                        # print("end_line_2",end_line_2)
                        
                        # obtain code segment of func2
                        idx_ln_4 = idx_ln_3+2
                        code_2 = ""
                        while "</source>" not in lines_2[idx_ln_4]:
                            code_2 += lines_2[idx_ln_4]
                            idx_ln_4 += 1
                        code_2 = code_2.replace("\n","").replace(" ","").replace("INDENT",";").replace("DEDENT",";")
                        # print("code_2:\n{}".format(code_2))
                        # print(code_2 in code_1)
                        if code_2 in code_1 and int(start_line_2) >= int(start_line) and int(end_line_2) <= int(end_line):
                            function_in_xml[1].append(int(start_line_2))
                            function_in_xml[2].append(int(end_line_2))
                            function_in_xml[0].append(py_path)
                            function_in_xml[3].append(func_1_name)
                            function_in_xml[4].append(func_2_name)
                            # print("code_2 found for {}:\n{}".format(func_2_name,code_2))
                            # break
            idx_ln_2 += 1

print("{} py_path, {} start_line, {} end_line, {} func_1_name, {} func_2_name  found in the .xml file"
      .format(len(function_in_xml[0]),len(function_in_xml[1]),len(function_in_xml[2]),len(function_in_xml[3]),len(function_in_xml[4])))
print()
print("The first 5 records are:")
for i in range(5):
    print(function_in_xml[0][i], function_in_xml[1][i], function_in_xml[2][i], function_in_xml[3][i], function_in_xml[4][i])
print()

# check how many rows not assigned before processing
print(Counter(list(code_metric_function_2['function_location'])))
print()

# the format of the function name in the dataframe is py_file_name.func1_name.func2_name

for idx in range(len(function_in_xml[4])):
    py_path = function_in_xml[0][idx]
    py_file_name = py_path[get_last_index(py_path,'/')+1:py_path.index(".py")]
    func_1_name = function_in_xml[3][idx]
    # print(func_1_name)
    func_1_name = func_1_name[:func_1_name.index('(')] if '(' in func_1_name else func_1_name
    func_2_name = function_in_xml[4][idx].replace('(','\(').replace(')','\)').replace('*','\*')
    
    # print(repr(py_file_name), repr(func_1_name), repr(func_2_name))
    
    # look for the dataframe rows that contain both py_file_name and func_name
    dataset_tmp = code_metric_function_2[(code_metric_function_2['Name'].str.contains(py_file_name)) & 
                                         (code_metric_function_2['Name'].str.contains(func_1_name)) &
                                         (code_metric_function_2['Name'].str.contains(func_2_name))]
    # print(dataset_tmp.shape[0])
    if dataset_tmp.shape[0] == 1: 
        code_metric_function_2.loc[dataset_tmp.index,'function_location'] = function_in_xml[0][idx]
        code_metric_function_2.loc[dataset_tmp.index,'StartLine'] = function_in_xml[1][idx]
        code_metric_function_2.loc[dataset_tmp.index,'EndLine'] = function_in_xml[2][idx]
    elif dataset_tmp.shape[0] == 0:
        print("Not found, rows have {}, {}, and {}".format(py_file_name, function_in_xml[3][idx], function_in_xml[4][idx]))
    else:
        print("Multiple rows have {}, {}, and {}".format(py_file_name, function_in_xml[3][idx], function_in_xml[4][idx]))

# check how many rows not changed
print()
print(Counter(list(code_metric_function_2['function_location'])))


for idx in range(len(function_in_xml[4])):
    py_path = function_in_xml[0][idx]
    py_file_name = py_path[get_last_index(py_path,'/')+1:py_path.index(".py")]
    func_1_name = function_in_xml[3][idx]
    # print(func_1_name)
    func_1_name = func_1_name[:func_1_name.index('(')] if '(' in func_1_name else func_1_name
    func_2_name = function_in_xml[4][idx].replace('(','\(').replace(')','\)').replace('*','\*')
    
    # print(repr(py_file_name), repr(func_1_name), repr(func_2_name))
    
    # look for the dataframe rows that contain both py_file_name and func_name
    dataset_tmp = code_metric_function_2[(code_metric_function_2['Name'].str.contains(py_file_name)) & 
                                         (code_metric_function_2['Name']==func_1_name) &
                                         (code_metric_function_2['Name']==func_2_name) &
                                         (code_metric_function_2['function_location']=='not_assigned')]
    # print(dataset_tmp.shape[0])
    if dataset_tmp.shape[0] == 1: 
        code_metric_function_2.loc[dataset_tmp.index,'function_location'] = function_in_xml[0][idx]
        code_metric_function_2.loc[dataset_tmp.index,'StartLine'] = function_in_xml[1][idx]
        code_metric_function_2.loc[dataset_tmp.index,'EndLine'] = function_in_xml[2][idx]
    elif dataset_tmp.shape[0] == 0:
        print("Not found, rows have {}, {}, and {}".format(py_file_name, function_in_xml[3][idx], function_in_xml[4][idx]))
    else:
        print("Multiple rows have {}, {}, and {}".format(py_file_name, function_in_xml[3][idx], function_in_xml[4][idx]))

# check how many rows not changed
print()
print(Counter(list(code_metric_function_2['function_location'])))

code_metric_function_2.head()

# case 3 - use the py files and xml file to check the remaining functions

dataset_tmp = code_metric_function_2[code_metric_function_2['function_location']=='not_assigned']['Name']
func_to_check = list(dataset_tmp)
print("The functions that need to further check: {}\n{}".format(len(func_to_check),func_to_check[0:5]))
print()
func_to_check = [_[get_last_index(_,'.')+1:] for _ in func_to_check]
print("The names of the functions to check: {}\n{}".format(len(func_to_check),func_to_check[0:5]))
print()
func_in_xml_lines = {}
for idx_func in range(len(func_to_check)):
    if "," in func_to_check[idx_func]:
        _ = func_to_check[idx_func]
        _ = _.replace('(',',').replace(')',',')
        list_tmp = _.split(',')
        # print(list_tmp)
        func_in_xml_lines[func_to_check[idx_func]]=[]
        f = open(function_xml_path, "r", encoding='utf-8')
        lines = f.readlines()
        f.close()
        for idx_ln in range(len(lines)):
            if "def" in lines[idx_ln] and all_list_string_in_one_string_line(lines[idx_ln], list_tmp):
                # print("{} in line {}: {}".format(list_tmp, idx_ln, lines[idx_ln]))
                func_in_xml_lines[func_to_check[idx_func]].append(idx_ln)           
    else:
        # print(func_to_check[idx_func])
        func_in_xml_lines[func_to_check[idx_func]]=[]
        f = open(function_xml_path, "r", encoding='utf-8')
        lines = f.readlines()
        f.close()
        for idx_ln in range(len(lines)):
            if "def" in lines[idx_ln] and func_to_check[idx_func] in lines[idx_ln]:
                # print("{} in line {}: {}".format(func_to_check[idx_func], idx_ln, lines[idx_ln]))
                func_in_xml_lines[func_to_check[idx_func]].append(idx_ln)

function_to_verify_by_py_and_xml = []
for func in list(dataset_tmp):
    func_ = func[get_last_index(func,'.')+1:]
    if len(func_in_xml_lines[func_]) != 0:
        # print("{}:{}".format(func,func_in_xml_lines[func_]))
        function_to_verify_by_py_and_xml.append(func)
        # open the .py files in directories, obtain the startline and then compare to that in the .xml file
print("The functions that might be further parsed: {}\n{}".format(len(function_to_verify_by_py_and_xml), function_to_verify_by_py_and_xml[0:5]))
print()
        
# obtain all the .py files in the directory
# path_to_walk = "C:\\Users\\yoyo.yao\\Desktop\\Job_Lakehead University_Lab Technologist\\MSC_CS\\Lakehead_COMP5413_Special Topics on Software Engineering\\project4\\tormado-master"
path_to_walk = MAIN_PATH + "code_metrics/py/tornado-master"

path_file = []

for root, dirs, files in os.walk(path_to_walk, topdown=False):
    for _ in files:
        path_file.append(os.path.join(root, _))

py_file = [_ for _ in path_file if ".py" in _]
print("{} .py files in total".format(len(py_file)))
print("The first 5 .py files:\n{}".format(py_file[0:5]))

for func in function_to_verify_by_py_and_xml:
    # seprate the path, function name, and arguments
    # idx = get_last_index(func,".")
    # func_path = func[:idx].split(".")
    str1 = func
    idx = get_last_index(str1,".")
    str1 = str1[:idx]
    str1 = str1[:get_first_uppercase_index(str1)] if get_first_uppercase_index(str1) != None else str1
    func_path = str1.replace("."," ").split()    
    # print(str1)        
    func_name = func[idx+1:][:func[idx+1:].index('(')]
    func_name = "def " + func_name 
    func_argu = func[idx+1:][func[idx+1:].index('(')+1:].replace(',',' ').replace(')',' ').split()
    
    for py in py_file:
        # if the func path seems to match the py file path, open this py file and check
        if all_list_string_in_one_string_line(py, func_path):
            f_py = open(py, "r", encoding='utf-8')
            lines_py = f_py.readlines()
            f_py.close()
            # if the function name and arguments are all in the line, retrieve the line number and read xml file to compare
            # if the startline number matches and only 1 match is found, we assume the function is found
            for idx_ln_py in range(len(lines_py)):
                if not all_list_string_in_one_string_line(lines_py[idx_ln_py], ["def","(",")",":"]):
                    continue
                if any_list_string_in_one_string_line(lines_py[idx_ln_py], ["-","{","}","@","default"]):
                    continue
                # print(lines_py[idx_ln_py])
                if func_name != lines_py[idx_ln_py][lines_py[idx_ln_py].index("def"):lines_py[idx_ln_py].index("(")]:
                    continue
                if len(func_argu) == 0 or (len(func_argu) != 0 and all_list_string_in_one_string_line(lines_py[idx_ln_py], func_argu)):
                    # retrieve the start line number in .py file
                    start_line_py = idx_ln_py + 1
                    # check the information in the xml file
                    f_xml = open(function_xml_path, "r", encoding='utf-8')
                    lines_xml = f_xml.readlines()
                    f_xml.close()
                    function_match_found = []
                    for idx_ln_xml in range(len(lines_xml)):
                        # if the function name and arguments are all in the line of xml file, retrieve the line number
                        if not all_list_string_in_one_string_line(lines_xml[idx_ln_xml], ["def","(",")",":"]):
                            continue
                        if any_list_string_in_one_string_line(lines_xml[idx_ln_xml], ["-","{","}","@","default"]):
                            continue
                        # print(lines_xml[idx_ln_xml])
                        if func_name != lines_xml[idx_ln_xml][lines_xml[idx_ln_xml].index("def"):lines_xml[idx_ln_xml].index("(")]:
                            continue
                        if (len(func_argu) == 0 or (len(func_argu) != 0 \
                            and all_list_string_in_one_string_line(lines_xml[idx_ln_xml], func_argu))) \
                            and "<source file=" in lines_xml[idx_ln_xml-1]:
                            ln_xml_func = lines_xml[idx_ln_xml-1]
                            # print(ln_xml_func)
                            ln_xml_func = ln_xml_func[ln_xml_func.index('startline="')+len('startline="'):]
                            start_line_xml = int(ln_xml_func[:ln_xml_func.index('"')])
                            if start_line_xml == start_line_py:
                                ln_xml_func = lines_xml[idx_ln_xml-1]
                                ln_xml_func = ln_xml_func[ln_xml_func.index('tornado-master'):]
                                func_path_xml = ln_xml_func[:ln_xml_func.index('"')]
                                ln_xml_func = lines_xml[idx_ln_xml-1]
                                ln_xml_func = ln_xml_func[ln_xml_func.index('endline="')+len('endline="'):]
                                end_line_xml = int(ln_xml_func[:ln_xml_func.index('"')])
                                function_match_found.append([func_path_xml, start_line_xml, end_line_xml])
                    # after traverse each line in the xml file, decide whether the result is unique
                    # if yes, assign it to the dataframe
                    if len(function_match_found)==1:
                        print("Match found for {} at line {} in {} and {}".format(func, start_line_py, py, lines_xml[idx_ln_xml-1]))
                        # assign values to dataframe
                        # look for the dataframe rows that contains the specific function name and has not been assigned
                        dataset_tmp = code_metric_function_2[(code_metric_function_2['Name']==func) &
                                                             (code_metric_function_2['function_location']=='not_assigned')]
                        if dataset_tmp.shape[0] == 1: 
                            code_metric_function_2.loc[dataset_tmp.index,'function_location'] = function_match_found[0][0]
                            code_metric_function_2.loc[dataset_tmp.index,'StartLine'] = function_match_found[0][1]
                            code_metric_function_2.loc[dataset_tmp.index,'EndLine'] = function_match_found[0][2]
                        elif dataset_tmp.shape[0] == 0:
                            print("Not found, rows have {}".format(func))
                        else:
                            print("Multiple rows have {}".format(func))    
                            
                    elif len(function_match_found)>1:
                        print("Multiple matches found for {} at {}".format(func, function_match_found))
                    else:
                        print("No matches found for {}".format(func))
                            
# check how many rows not changed
print()
print(Counter(list(code_metric_function_2['function_location'])))

code_metric_function_2.head()                           
                    
            
            
tornado_dataset = code_metric_function_2[code_metric_function_2['function_location']!='not_assigned']

print("There are {} instances.".format(tornado_dataset.shape[0]))

tornado_dataset.head()


tornado_dataset['is_cloned']=[0 for _ in range(tornado_dataset.shape[0])]
tornado_dataset = tornado_dataset[['function_location',
                                                 'Name',
                                                 'StartLine',
                                                 'EndLine',
                                                 'CountLine', 
                                                 'CountLineBlank', 
                                                 'CountLineCode', 
                                                 'CountLineCodeDecl', 
                                                 'CountLineCodeExe', 
                                                 'CountLineComment', 
                                                 'CountPath', 
                                                 'CountPathLog', 
                                                 'CountStmt', 
                                                 'CountStmtDecl', 
                                                 'CountStmtExe', 
                                                 'Cyclomatic', 
                                                 'CyclomaticModified', 
                                                 'CyclomaticStrict', 
                                                 'Essential', 
                                                 'MaxNesting', 
                                                 'RatioCommentToCode',
                                                 'is_cloned']]
tornado_dataset.head()


# parse the functions.xml file to locate the functions
# function_clone_xml_path = ["C:\\Users\\yoyo.yao\\Desktop\\Job_Lakehead University_Lab Technologist\\MSC_CS\\Lakehead_COMP5413_Special Topics on Software Engineering\\project4\\tornado\\type1_tornado-master_functions-clones\\tornado-master_functions-clones-0.00.xml",
#                            "C:\\Users\\yoyo.yao\\Desktop\\Job_Lakehead University_Lab Technologist\\MSC_CS\\Lakehead_COMP5413_Special Topics on Software Engineering\\project4\\tornado\\type2_tornado-master_functions-blind-clones\\tornado-master_functions-blind-clones-0.00.xml",
#                            "C:\\Users\\yoyo.yao\\Desktop\\Job_Lakehead University_Lab Technologist\\MSC_CS\\Lakehead_COMP5413_Special Topics on Software Engineering\\project4\\tornado\\type2c_tornado-master_functions-consistent-clones\\tornado-master_functions-consistent-clones-0.00.xml",
#                            "C:\\Users\\yoyo.yao\\Desktop\\Job_Lakehead University_Lab Technologist\\MSC_CS\\Lakehead_COMP5413_Special Topics on Software Engineering\\project4\\tornado\\type3-1_tornado-master_functions-clones\\tornado-master_functions-clones-0.30.xml",
#                            "C:\\Users\\yoyo.yao\\Desktop\\Job_Lakehead University_Lab Technologist\\MSC_CS\\Lakehead_COMP5413_Special Topics on Software Engineering\\project4\\tornado\\type3-2_tornado-master_functions-blind-clones\\tornado-master_functions-blind-clones-0.30.xml",
#                            "C:\\Users\\yoyo.yao\\Desktop\\Job_Lakehead University_Lab Technologist\\MSC_CS\\Lakehead_COMP5413_Special Topics on Software Engineering\\project4\\tornado\\type3-2c_tornado-master_functions-consistent-clones\\tornado-master_functions-consistent-clones-0.30.xml"]

function_clone_xml_path = [MAIN_PATH + "code_metrics/xml/tornado/type1_tornado-master_functions-clones/tornado-master_functions-clones-0.00.xml",
                           MAIN_PATH + "code_metrics/xml/tornado/type2_tornado-master_functions-blind-clones/tornado-master_functions-blind-clones-0.00.xml",
                           MAIN_PATH + "code_metrics/xml/tornado/type2c_tornado-master_functions-consistent-clones/tornado-master_functions-consistent-clones-0.00.xml",
                           MAIN_PATH + "code_metrics/xml/tornado/type3-1_tornado-master_functions-clones/tornado-master_functions-clones-0.30.xml",
                           MAIN_PATH + "code_metrics/xml/tornado/type3-2_tornado-master_functions-blind-clones/tornado-master_functions-blind-clones-0.30.xml",
                           MAIN_PATH + "code_metrics/xml/tornado/type3-2c_tornado-master_functions-consistent-clones/tornado-master_functions-consistent-clones-0.30.xml"]

for xml_path in function_clone_xml_path:
    # read lines of each xml file
    f = open(xml_path, "r", encoding='utf-8')
    lines = f.readlines()
    f.close()
    
    # parse the line and assign the label to corresponding row
    for idx in range(len(lines)):
        ln = lines[idx]
        if "<source file=" in ln:
            ln = ln[ln.index("tornado-master"):]
            func_loc = ln[:ln.index('"')]
            ln = ln[ln.index('startline="')+len('startline="'):]
            start_line = int(ln[:ln.index('"')])
            ln = ln[ln.index('endline="')+len('endline="'):]
            end_line = int(ln[:ln.index('"')])
            # print(func_loc, start_line, end_line)
            
            # update the dataframe
            # look for the dataframe rows that contain both py_file_name and func_name
            dataset_tmp = tornado_dataset[(tornado_dataset['function_location']==func_loc) &
                                        (tornado_dataset['StartLine']==start_line) &
                                        (tornado_dataset['EndLine']==end_line) &
                                        (tornado_dataset['is_cloned']==0)]
            
            if dataset_tmp.shape[0] == 1:
                tornado_dataset.loc[dataset_tmp.index,'is_cloned'] = 1
    print("Parsing cloned functions is completed for {}.".format(xml_path))

# check how many rows not changed
print()
print(Counter(list(tornado_dataset['is_cloned'])))

tornado_dataset.head()

dataset_1 = tornado_dataset[['CountLine', 
                             'CountLineBlank', 
                             'CountLineCode', 
                             'CountLineCodeDecl', 
                             'CountLineCodeExe', 
                             'CountLineComment', 
                             'CountPath', 
                             'CountPathLog', 
                             'CountStmt', 
                             'CountStmtDecl', 
                             'CountStmtExe', 
                             'Cyclomatic', 
                             'CyclomaticModified', 
                             'CyclomaticStrict', 
                             'Essential', 
                             'MaxNesting', 
                             'RatioCommentToCode',
                             'is_cloned']]
dataset_1.head()

# count the values occurrence in each column
for _ in dataset_1.columns.tolist():
  print(_)
  print(Counter(dataset_1[_]))

# save and load dataframe
dataframe_tornado_path = "/content/drive/MyDrive/Software_data/code_metrics/dataframe_tornado.pkl"

dataset_1.to_pickle(dataframe_tornado_path) 

dataset_1 = pd.read_pickle(dataframe_tornado_path)

# apply smote to address the dataset imbalance issue
dataset_1 = shuffle(dataset_1)
dataset_1_X = dataset_1.iloc[:,0:dataset_1.shape[1]-1]
print("dataset_1_X has a shape of {}".format(dataset_1_X.shape))
print("the first 5 instances:\n{}".format(dataset_1_X.iloc[0:5,:]))
dataset_1_y = dataset_1.iloc[:,-1:]
print("dataset_1_y has a shape of {}".format(dataset_1_y.shape))
print("the first 5 instances:\n{}".format(dataset_1_y.iloc[0:5,:]))
print()

print("Before applying SMOTE:")
print(Counter(dataset_1['is_cloned']))

dataset_1_X_smote, dataset_1_y_smote = SMOTE().fit_resample(dataset_1_X, dataset_1_y)

print("After applying SMOTE:")
print(Counter(dataset_1_y_smote))

dataset_1_X = pd.DataFrame(data=dataset_1_X_smote,    # values
                  #   index=data[1:,0],    # 1st column as index
                      columns=dataset_1_X.columns.tolist())  # 1st row as the column names
dataset_1_y = pd.DataFrame(data=dataset_1_y_smote,    # values
                  #   index=data[1:,0],    # 1st column as index
                      columns=dataset_1_y.columns.tolist())  # 1st row as the column names

# split into train test sets
dataset_1_X_train, dataset_1_X_test, dataset_1_y_train, dataset_1_y_test = train_test_split(dataset_1_X, dataset_1_y, test_size=0.2, stratify = dataset_1_y)
np.save(MAIN_PATH+"dataset/tornado_X_train.npy",np.array(dataset_1_X_train))
np.save(MAIN_PATH+"dataset/tornado_X_test.npy",np.array(dataset_1_X_test))
np.save(MAIN_PATH+"dataset/tornado_y_train.npy",np.array(dataset_1_y_train).flatten())
np.save(MAIN_PATH+"dataset/tornado_y_test.npy",np.array(dataset_1_y_test).flatten())
# dataset_1_X_train_back = dataset_1_X_train.copy()
# dataset_1_X_test_back = dataset_1_X_test.copy()

# verify the class distribution
print("class 0:1 in dataset_1_y is       : {}:{}".format(Counter(list(dataset_1_y['is_cloned']))[0]/dataset_1_y.shape[0], 1-Counter(list(dataset_1_y['is_cloned']))[0]/dataset_1_y.shape[0]))
print("class 0:1 in dataset_1_y_train is : {}:{}".format(Counter(list(dataset_1_y_train['is_cloned']))[0]/dataset_1_y_train.shape[0], 1-Counter(list(dataset_1_y_train['is_cloned']))[0]/dataset_1_y_train.shape[0]))
print("class 0:1 in dataset_1_y_test is  : {}:{}".format(Counter(list(dataset_1_y_test['is_cloned']))[0]/dataset_1_y_test.shape[0], 1-Counter(list(dataset_1_y_test['is_cloned']))[0]/dataset_1_y_test.shape[0]))
print()

# the train set will be further split by stratified splitting for cross validation
skf = StratifiedKFold(n_splits=5)
skf_tr_va_sets = skf.split(dataset_1_X_train, dataset_1_y_train)
for idx_f, (idx_tr, idx_va) in enumerate(skf_tr_va_sets):
  tr_X = dataset_1_X.iloc[idx_tr]
  tr_y = dataset_1_y.iloc[idx_tr]
  va_X = dataset_1_X.iloc[idx_va]
  va_y = dataset_1_y.iloc[idx_va]
  print("Fold {}\ntrain set X:{} train set y: {} validation set X:{} validation set y: {}".format(idx_f,tr_X.shape,tr_y.shape,va_X.shape,va_y.shape))
  print("class 0:1 in train set y is      : {}:{}".format(Counter(list(tr_y['is_cloned']))[0]/tr_y.shape[0], 1-Counter(list(tr_y['is_cloned']))[0]/tr_y.shape[0]))
  print("class 0:1 in validation set y is : {}:{}".format(Counter(list(va_y['is_cloned']))[0]/va_y.shape[0], 1-Counter(list(va_y['is_cloned']))[0]/va_y.shape[0]))

"""## 3.2 Train and Test - Original Datasest

### 3.2.1 DT
"""

save_path = MAIN_PATH + "model/"
data_type = "tornado"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='dt')

"""### 3.2.2 RF"""

save_path = MAIN_PATH + "model/"
data_type = "tornado"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='rf')

"""### 3.2.3 KNN"""

save_path = MAIN_PATH + "model/"
data_type = "tornado"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='knn')

"""### 3.2.4 NB"""

save_path = MAIN_PATH + "model/"
data_type = "tornado"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nb')

"""### 3.2.5 LR"""

save_path = MAIN_PATH + "model/"
data_type = "tornado"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lr')

"""### 3.2.6 LDA"""

save_path = MAIN_PATH + "model/"
data_type = "tornado"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lda')

"""### 3.2.7 QDA"""

save_path = MAIN_PATH + "model/"
data_type = "tornado"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='qda')

"""### 3.2.8 SVM"""

save_path = MAIN_PATH + "model/"
data_type = "tornado"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='svm')

"""### 3.2.9 MLP"""

save_path = MAIN_PATH + "model/"
data_type = "tornado"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='mlp')

"""### 3.2.10 NN"""

save_path = MAIN_PATH + "model/"
data_type = "tornado"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nn')

"""## 3.3 Feature Selection

by Accuracy
"""

# use RF to select features
data_type = "tornado"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")

model = RandomForestClassifier(max_depth=None, random_state=0)
feature_percentage = 1.0

# Sequential Forward Selection by accuracy
sfs_acc = SFS(model, 
          k_features=int(X_train.shape[1]*feature_percentage),   # select 80% feature columns
          forward=True, 
          floating=False, 
          verbose=2,
          scoring='accuracy',
          cv=5,                                 # cross validation
          n_jobs=-1)                            # run CV on all CPU cores

sfs_acc = sfs_acc.fit(X_train, y_train)

"""by F1-score"""

# use RF to select features
data_type = "tornado"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")

model = RandomForestClassifier(max_depth=None, random_state=0)
feature_percentage = 1.0

# Sequential Forward Selection by accuracy
sfs_f1 = SFS(model, 
          k_features=int(X_train.shape[1]*feature_percentage),   # select 80% feature columns
          forward=True, 
          floating=False, 
          verbose=2,
          scoring='f1',
          cv=5,                                 # cross validation
          n_jobs=-1)                            # run CV on all CPU cores

sfs_f1 = sfs_f1.fit(X_train, y_train)

# select feature that increase average cv score by accuracy
print('Sequential Forward Selection (k={}) by Accuracy'.format(int(X_train.shape[1]*feature_percentage)))
print("Indices of selected features: {}".format(sfs_acc.k_feature_idx_))
print('CV Score                    : {}'.format(sfs_acc.k_score_))

data_type = "tornado"
feature_name = pd.read_pickle(MAIN_PATH + "code_metrics/dataframe_"+data_type+".pkl").columns.tolist()
feature_name = feature_name[:-1]

feature_name_selected = [feature_name[idx] for idx in sfs_acc.k_feature_idx_]
print("Selected features           : {}".format(feature_name_selected))

feature_idx_selected_by_sequence = []
feature_cv_avg_score = []
for _ in sfs_acc.subsets_:
  # print(sfs_acc.subsets_[_])
  for idx in sfs_acc.subsets_[_]['feature_names']:
    if int(idx) not in feature_idx_selected_by_sequence:
      feature_idx_selected_by_sequence.append(int(idx))
      feature_cv_avg_score.append(sfs_acc.subsets_[_]['avg_score'])

print("Feature indices selected by sequence               : {}".format(feature_idx_selected_by_sequence))
print("CV average score by current feature selection      : {}".format(feature_cv_avg_score))

# select only the features keep increasing the CV average score
target_idx = 0
for idx in range(len(feature_cv_avg_score)):
  if idx+1 == len(feature_cv_avg_score) or feature_cv_avg_score[idx+1] <= feature_cv_avg_score[idx]:
    target_idx = idx
    break

feature_idx_increase_cv_score =  feature_idx_selected_by_sequence[:idx+1]
print("The feature indices that increase CV average score : {}".format(feature_idx_increase_cv_score))
feature_name_increase_cv_score = [feature_name[idx] for idx in feature_idx_increase_cv_score]
print("The feature names that increase CV average score   : {}".format(feature_name_increase_cv_score))
feature_idx_selected_by_acc = feature_idx_increase_cv_score

# select feature that increase average cv score by f1-score
print()
print('Sequential Forward Selection (k={}) by F1-score'.format(int(X_train.shape[1]*feature_percentage)))
print("Indices of selected features: {}".format(sfs_f1.k_feature_idx_))
print('CV Score                    : {}'.format(sfs_f1.k_score_))

data_type = "tornado"
feature_name = pd.read_pickle(MAIN_PATH + "code_metrics/dataframe_"+data_type+".pkl").columns.tolist()
feature_name = feature_name[:-1]

feature_name_selected = [feature_name[idx] for idx in sfs_f1.k_feature_idx_]
print("Selected features           : {}".format(feature_name_selected))

feature_idx_selected_by_sequence = []
feature_cv_avg_score = []
for _ in sfs_f1.subsets_:
  # print(sfs_f1.subsets_[_])
  for idx in sfs_f1.subsets_[_]['feature_names']:
    if int(idx) not in feature_idx_selected_by_sequence:
      feature_idx_selected_by_sequence.append(int(idx))
      feature_cv_avg_score.append(sfs_f1.subsets_[_]['avg_score'])

print("Feature indices selected by sequence               : {}".format(feature_idx_selected_by_sequence))
print("CV average score by current feature selection      : {}".format(feature_cv_avg_score))

# select only the features keep increasing the CV average score
target_idx = 0
for idx in range(len(feature_cv_avg_score)):
  if idx+1 == len(feature_cv_avg_score) or feature_cv_avg_score[idx+1] <= feature_cv_avg_score[idx]:
    target_idx = idx
    break

feature_idx_increase_cv_score =  feature_idx_selected_by_sequence[:idx+1]
print("The feature indices that increase CV average score : {}".format(feature_idx_increase_cv_score))
feature_name_increase_cv_score = [feature_name[idx] for idx in feature_idx_increase_cv_score]
print("The feature names that increase CV average score   : {}".format(feature_name_increase_cv_score))
feature_idx_selected_by_f1 = feature_idx_increase_cv_score

# keep the intersection of two lists
feature_idx_increase_cv_score = list(set(feature_idx_selected_by_acc) & set(feature_idx_selected_by_f1))

# build the feature set using the selected features
data_type = "tornado"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

data_type = "tornado_sfs"
np.save(MAIN_PATH+"dataset/"+data_type+"_X_train.npy",X_train[:,feature_idx_increase_cv_score])
np.save(MAIN_PATH+"dataset/"+data_type+"_y_train.npy",y_train)
np.save(MAIN_PATH+"dataset/"+data_type+"_X_test.npy",X_test[:,feature_idx_increase_cv_score])
np.save(MAIN_PATH+"dataset/"+data_type+"_y_test.npy",y_test)

"""## 3.4 Train and Test - SFS Dataset

### 3.4.1 DT
"""

save_path = MAIN_PATH + "model/"
data_type = "tornado_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='dt')

"""### 3.4.2 RF"""

save_path = MAIN_PATH + "model/"
data_type = "tornado_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='rf')

"""### 3.4.3 KNN"""

save_path = MAIN_PATH + "model/"
data_type = "tornado_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='knn')

"""### 3.4.4 NB"""

save_path = MAIN_PATH + "model/"
data_type = "tornado_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nb')

"""### 3.4.5 LR"""

save_path = MAIN_PATH + "model/"
data_type = "tornado_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lr')

"""### 3.4.6 LDA"""

save_path = MAIN_PATH + "model/"
data_type = "tornado_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lda')

"""### 3.4.7 QDA"""

save_path = MAIN_PATH + "model/"
data_type = "tornado_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='qda')

"""### 3.4.8 SVM"""

save_path = MAIN_PATH + "model/"
data_type = "tornado_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='svm')

"""### 3.4.9 MLP"""

save_path = MAIN_PATH + "model/"
data_type = "tornado_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='mlp')

"""### 3.4.10 NN"""

save_path = MAIN_PATH + "model/"
data_type = "tornado_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nn')

"""## 3.5 Ensemble"""

save_path = MAIN_PATH + "model/"
data_type = "tornado"
metrics = os.listdir(save_path)

# select classifiers by thresholds
th_acc = 0.7
th_f1s = 0.7
clf_sel = []

print("Results:")
print("-"*80)
metrics_title = ['Accuracy','F1-score','Precision','Recall','Confusion Matrix']
print(" "*45,end="")
for m_t in metrics_title:
  print("{: >11} ".format(m_t),end="")
print()
# list label encoding classifiers
idx_clf = 0
for me in metrics:
  if "metric" in me:
    if data_type in me:    # model_le_smote_svm.pkl, true_le_smote_svm.npy, pred_le_smote_svm.npy, metric_le_smote_svm.npy
      idx_clf += 1
      clf_name = me[len("metric_"):len(me)-len(".npy")] 
      print("Classifier #{:2d}: {:20s} ".format(idx_clf, clf_name), end="")
      me_list = np.load(save_path+me)
      me_list = me_list.tolist()
      print("Metrics:",end="")
      for m_l in me_list:
        print("{: >11} ".format("{:4.7f}".format(m_l)),end="")
      print()

      # select the classifier if it meets thresholds
      if me_list[0]>=th_acc and me_list[1]>=th_f1s:
        clf_sel.append(clf_name)

print("{} classifiers selected:{}\n".format(len(clf_sel),clf_sel))
if len(clf_sel) > 0:
  # output the true and predictions of each selected classifier
  for i in range(len(clf_sel)):
    print("classifier #{:2d}".format(i+1))
    print("True      :",np.load(save_path+"true_"+clf_sel[i]+".npy").astype(np.uint8).tolist())
    print("Prediction:",np.load(save_path+"pred_"+clf_sel[i]+".npy").astype(np.uint8).tolist())

  # check whether the saved true values are the same as the original values
  flask_y_test = np.load(MAIN_PATH+"dataset/"+"flask_y_test.npy")

  # check whether each saved y values match the label encoding y values
  for i in range(len(clf_sel)):
    true = np.load(save_path+"true_"+clf_sel[i]+".npy")
    for j in range(len(true)):
      if list(flask_y_test)[j] - true.flatten()[j] !=0:
        print("y values of label encoding does not match those of saved y values for {}".format(clf_sel[i]))
        break
      elif j == len(true)-1:
        print("y values of label encoding match those of saved y vaues for {}".format(clf_sel[i]))  
      
  # generate the prediction values by voting
  pred = []
  for i in range(len(clf_sel)):
    pred.append(np.load(save_path+"pred_"+clf_sel[i]+".npy").astype(np.uint8).tolist())
  pred = np.array(pred)
  pred_ensemble = []
  for i in range(pred.shape[1]):
    if sum(pred[:,i])>=pred.shape[0]-sum(pred[:,i]):
      pred_ensemble.append(1)
    else:
      pred_ensemble.append(0)
  true = np.load(save_path+"true_"+clf_sel[0]+".npy").astype(np.uint8).tolist()
  print("True                :",true)
  print("Prediction by voting:",pred_ensemble)

  print("Accuracy score             :",accuracy_score(true, pred_ensemble))
  print("F1-score                   :",f1_score(true, pred_ensemble))
  print("Precision score            :",precision_score(true, pred_ensemble))
  print("Recall score               :",recall_score(true, pred_ensemble))
  print("Classification report      :\n",classification_report(true, pred_ensemble))
  print("Confusion Matrix           :\n",confusion_matrix(true, pred_ensemble))

  # plot confusion matrix
  cm = confusion_matrix(true, pred_ensemble)
  plot_confusion_matrix(cm, normalize = True, target_names = ["not readmitted","readmitted"], title = "Confusion Matrix")

else:
  print("No reliable results!")

"""# 4\. Django

## 4.1 Extracting data - the same procedures as that of Flask, not to show details
"""

# code_metric = "/content/drive/MyDrive/Software_data/code_metrics/csv/django-main.csv"
code_metric = MAIN_PATH + "code_metrics/csv/django-main.csv"
code_metric_dataset = pd.read_csv(code_metric)
# code_metric_dataset.head()

code_metric_function = code_metric_dataset[code_metric_dataset['Kind']=='Function']
# code_metric_function.head()

code_metric_function = code_metric_function[['Name', 
                                             'CountLine', 
                                             'CountLineBlank', 
                                             'CountLineCode', 
                                             'CountLineCodeDecl', 
                                             'CountLineCodeExe', 
                                             'CountLineComment', 
                                             'CountPath', 
                                             'CountPathLog', 
                                             'CountStmt', 
                                             'CountStmtDecl', 
                                             'CountStmtExe', 
                                             'Cyclomatic', 
                                             'CyclomaticModified', 
                                             'CyclomaticStrict', 
                                             'Essential', 
                                             'MaxNesting', 
                                             'RatioCommentToCode']]
# code_metric_function.head()

code_metric_function_2 = code_metric_function
code_metric_function_2['function_location']=['not_assigned' for _ in range(code_metric_function.shape[0])]
code_metric_function_2['StartLine']=[-1 for _ in range(code_metric_function.shape[0])]
code_metric_function_2['EndLine']=[-1 for _ in range(code_metric_function.shape[0])]
code_metric_function_2 = code_metric_function_2[['function_location',
                                                 'Name',
                                                 'StartLine',
                                                 'EndLine',
                                                 'CountLine', 
                                                 'CountLineBlank', 
                                                 'CountLineCode', 
                                                 'CountLineCodeDecl', 
                                                 'CountLineCodeExe', 
                                                 'CountLineComment', 
                                                 'CountPath', 
                                                 'CountPathLog', 
                                                 'CountStmt', 
                                                 'CountStmtDecl', 
                                                 'CountStmtExe', 
                                                 'Cyclomatic', 
                                                 'CyclomaticModified', 
                                                 'CyclomaticStrict', 
                                                 'Essential', 
                                                 'MaxNesting', 
                                                 'RatioCommentToCode']]
code_metric_function_2.head()

# look for the function names in the dataframe, if there is no duplicated name, assign the values to the dataframe row

# case 1 - the function in the dataframe has no replicated records, totally matched by function names
# the format of the function name in the dataframe is py_file_name.x.func_name

# parse the functions.xml file to locate the functions
# function_xml_path = "/content/drive/MyDrive/Software_data/code_metrics/xml/django/type1_django-main_functions.xml"
function_xml_path = MAIN_PATH + "code_metrics/xml/django/type1_django-main_functions.xml"
f = open(function_xml_path, "r", encoding='utf-8')
lines = f.readlines()
f.close()

function_in_xml = [[], # store the py file location
                   [], # start line
                   [], # end line
                   []] # function name after processing

for idx_ln in range(len(lines)):
    # if (idx_ln + 1) % 1000 == 0:
    #   print("{:6d} / {:6d} lines completed!".format(idx_ln+1, len(lines)))
    ln = lines[idx_ln]
    if "<source file=" in ln:
        idx_st = ln.index("django-main")
        idx_ed = ln.index(".py")
        py_path = ln[idx_st:idx_ed+len(".py")]
        # print(py_path)
        idx_st = ln.index('startline="')
        ln = ln[idx_st+len('startline="'):]
        start_line = ln[:ln.index('"')]
        # print(start_line)
        idx_st = ln.index('endline="')
        ln = ln[idx_st+len('endline="'):]
        end_line = ln[:ln.index('"')]
        # print(end_line)
        ln = lines[idx_ln+1]
        if "def" in ln and "(" in ln and ")" not in ln and ":" not in ln:
          idx_next_line = idx_ln+2
          while ")" not in ln and ":" not in lines[idx_next_line]:
            ln += lines[idx_next_line]
            idx_next_line += 1
          ln += lines[idx_next_line] 
          ln = ln.replace("\n","")
          # print(ln)
        ln = ln[ln.index("def ")+len("def "):]
        # if idx_ln > 137000 and idx_ln<138000:
        #   print(ln)
        func_name = parse_function_name_line(ln)
        # print(func_name)
        function_in_xml[0].append(py_path)
        function_in_xml[1].append(int(start_line))
        function_in_xml[2].append(int(end_line))
        function_in_xml[3].append(func_name)    
        
print("{} functions found in the .xml file".format(len(function_in_xml[0])))
print()
print("The first 5 records are:")
for i in range(5):
    print(function_in_xml[0][i], function_in_xml[1][i], function_in_xml[2][i], function_in_xml[3][i])
print()
 
# check how many rows not assigned before processing
print(Counter(list(code_metric_function_2['function_location'])))
print()

for idx in range(len(function_in_xml[3])):
    py_path = function_in_xml[0][idx]
    py_file_name = py_path[get_last_index(py_path,'/')+1:py_path.index(".py")]
    func_name = function_in_xml[3][idx].replace('(','\(').replace(')','\)').replace('*','\*')
    
    # print(repr(py_file_name), repr(func_name))
    
    # look for the dataframe rows that contain both py_file_name and func_name
    dataset_tmp = code_metric_function_2[(code_metric_function_2['Name'].str.contains(py_file_name)) & 
                                         (code_metric_function_2['Name'].str.contains(func_name)) &
                                         (code_metric_function_2['function_location']=='not_assigned')]
    # print(dataset_tmp)
    if dataset_tmp.shape[0] == 1: 
        code_metric_function_2.loc[dataset_tmp.index,'function_location'] = function_in_xml[0][idx]
        code_metric_function_2.loc[dataset_tmp.index,'StartLine'] = function_in_xml[1][idx]
        code_metric_function_2.loc[dataset_tmp.index,'EndLine'] = function_in_xml[2][idx]
    elif dataset_tmp.shape[0] == 0:
        print("Not found, rows have {} and {}".format(py_file_name, function_in_xml[3][idx]))
    else:
        print("Multiple rows have {} and {}".format(py_file_name, function_in_xml[3][idx]))

# check how many rows not changed
print()
print(Counter(list(code_metric_function_2['function_location'])))

for idx in range(len(function_in_xml[3])):
    py_path = function_in_xml[0][idx]
    py_file_name = py_path[get_last_index(py_path,'/')+1:py_path.index(".py")]
    func_name = function_in_xml[3][idx].replace('(','\(').replace(')','\)').replace('*','\*')
    
    # print(repr(py_file_name), repr(func_name))
    
    # look for the dataframe rows that contain both py_file_name and func_name
    dataset_tmp = code_metric_function_2[(code_metric_function_2['Name'].str.contains(py_file_name)) & 
                                         (code_metric_function_2['Name']==func_name) &
                                         (code_metric_function_2['function_location']=='not_assigned')]
    # print(dataset_tmp)
    if dataset_tmp.shape[0] == 1: 
        code_metric_function_2.loc[dataset_tmp.index,'function_location'] = function_in_xml[0][idx]
        code_metric_function_2.loc[dataset_tmp.index,'StartLine'] = function_in_xml[1][idx]
        code_metric_function_2.loc[dataset_tmp.index,'EndLine'] = function_in_xml[2][idx]
    elif dataset_tmp.shape[0] == 0:
        print("Not found, rows have {} and {}".format(py_file_name, function_in_xml[3][idx]))
    else:
        print("Multiple rows have {} and {}".format(py_file_name, function_in_xml[3][idx]))

# check how many rows not changed
print()
print(Counter(list(code_metric_function_2['function_location'])))

code_metric_function_2.head()

# case 2 - the function is nested in another function
# the format of the function name in the dataframe is py_file_name.func1_name.func2_name

# parse the functions.xml file to locate the functions
# function_xml_path = "/content/drive/MyDrive/Software_data/code_metrics/xml/django/type1_django-main_functions.xml"
function_xml_path = MAIN_PATH + "code_metrics/xml/django/type1_django-main_functions.xml"
f = open(function_xml_path, "r", encoding='utf-8')
lines = f.readlines()
f.close()

function_in_xml = [[], # store the py file location
                   [], # start line of func2
                   [], # end line of func2
                   [], # func1 name after processing
                   []] # func2 name after processing

for idx_ln in range(len(lines)):
    ln = lines[idx_ln]
    if "<source file=" in ln:
        idx_st = ln.index("django-main")
        idx_ed = ln.index(".py")
        py_path = ln[idx_st:idx_ed+len(".py")]
        # print(py_path)
        idx_st = ln.index('startline="')
        ln = ln[idx_st+len('startline="'):]
        start_line = ln[:ln.index('"')]
        # print("start_line_1",start_line)
        idx_st = ln.index('endline="')
        ln = ln[idx_st+len('endline="'):]
        end_line = ln[:ln.index('"')]
        # print("end_line_1",end_line)
        ln = lines[idx_ln+1]
        if "def" in ln and "(" in ln and ")" not in ln and ":" not in ln:
          idx_next_line = idx_ln+2
          while ")" not in ln and ":" not in lines[idx_next_line]:
            ln += lines[idx_next_line]
            idx_next_line += 1
          ln += lines[idx_next_line] 
          ln = ln.replace("\n","")
          # print(ln)
        ln = ln[ln.index("def ")+len("def "):]
        func_1_name = parse_function_name_line(ln)
        # print(func_name)
        
        # obtain code segment of func1
        idx_ln_1 = idx_ln + 1
        code_1 = ""
        while "</source>" not in lines[idx_ln_1]: 
            code_1 += lines[idx_ln_1]
            idx_ln_1 += 1
        code_1 = code_1.replace("\n","").replace(" ","").replace("INDENT",";").replace("DEDENT",";")
        # print("code_1:\n{}".format(code_1))
        
        # obtain func2 names, strings stored in a list
        idx_ln_2 = idx_ln + 2   # the lines under the "def func_1_name()", which is idx_ln + 1
        while "</source>" not in lines[idx_ln_2]:
            if all_list_string_in_one_string_line(lines[idx_ln_2], ["def", "(", ":"]) and not \
               any_list_string_in_one_string_line(lines[idx_ln_2], ["default", "define", "@", "</p>", "."]):
            # if "def" in lines[idx_ln_2] and ":" in lines[idx_ln_2] and "default" not in lines[idx_ln_2] and "define" not in lines[idx_ln_2] and "@" not in lines[idx_ln_2]:
                ln_2 = lines[idx_ln_2]
                # print("ln_2:",ln_2)
                ln_2 = ln_2[ln_2.index("def ")+len("def "):]
                func_2_name = parse_function_name_line(ln_2)
                # obtain the startline and endline of func_2 by visiting the xml file again
                f_2 = open(function_xml_path, "r", encoding='utf-8')
                lines_2 = f_2.readlines()
                f_2.close()
                for idx_ln_3 in range(len(lines_2)):
                    if py_path in lines_2[idx_ln_3] and func_2_name in lines_2[idx_ln_3+1]:
                        # obtain startline and endline of func_2
                        ln_3 = lines_2[idx_ln_3]   # the line with <source file=
                        idx_st_2 = ln_3.index('startline="')
                        ln_3 = ln_3[idx_st_2+len('startline="'):]
                        start_line_2 = ln_3[:ln_3.index('"')]
                        # print("start_line_2",start_line_2)
                        idx_st_2 = ln_3.index('endline="')
                        ln_3 = ln_3[idx_st_2+len('endline="'):]
                        end_line_2 = ln_3[:ln_3.index('"')]
                        # print("end_line_2",end_line_2)
                        
                        # obtain code segment of func2
                        idx_ln_4 = idx_ln_3+2
                        code_2 = ""
                        while "</source>" not in lines_2[idx_ln_4]:
                            code_2 += lines_2[idx_ln_4]
                            idx_ln_4 += 1
                        code_2 = code_2.replace("\n","").replace(" ","").replace("INDENT",";").replace("DEDENT",";")
                        # print("code_2:\n{}".format(code_2))
                        # print(code_2 in code_1)
                        if code_2 in code_1 and int(start_line_2) >= int(start_line) and int(end_line_2) <= int(end_line):
                            function_in_xml[1].append(int(start_line_2))
                            function_in_xml[2].append(int(end_line_2))
                            function_in_xml[0].append(py_path)
                            function_in_xml[3].append(func_1_name)
                            function_in_xml[4].append(func_2_name)
                            # print("code_2 found for {}:\n{}".format(func_2_name,code_2))
                            # break
            idx_ln_2 += 1

print("{} py_path, {} start_line, {} end_line, {} func_1_name, {} func_2_name  found in the .xml file"
      .format(len(function_in_xml[0]),len(function_in_xml[1]),len(function_in_xml[2]),len(function_in_xml[3]),len(function_in_xml[4])))
print()
print("The first 5 records are:")
for i in range(5):
    print(function_in_xml[0][i], function_in_xml[1][i], function_in_xml[2][i], function_in_xml[3][i], function_in_xml[4][i])
print()

# check how many rows not assigned before processing
print(Counter(list(code_metric_function_2['function_location'])))
print()

# the format of the function name in the dataframe is py_file_name.func1_name.func2_name

for idx in range(len(function_in_xml[4])):
    py_path = function_in_xml[0][idx]
    py_file_name = py_path[get_last_index(py_path,'/')+1:py_path.index(".py")]
    func_1_name = function_in_xml[3][idx]
    # print(func_1_name)
    func_1_name = func_1_name[:func_1_name.index('(')] if '(' in func_1_name else func_1_name
    func_2_name = function_in_xml[4][idx].replace('(','\(').replace(')','\)').replace('*','\*')
    
    # print(repr(py_file_name), repr(func_1_name), repr(func_2_name))
    
    # look for the dataframe rows that contain both py_file_name and func_name
    dataset_tmp = code_metric_function_2[(code_metric_function_2['Name'].str.contains(py_file_name)) & 
                                         (code_metric_function_2['Name'].str.contains(func_1_name)) &
                                         (code_metric_function_2['Name'].str.contains(func_2_name))]
    # print(dataset_tmp.shape[0])
    if dataset_tmp.shape[0] == 1: 
        code_metric_function_2.loc[dataset_tmp.index,'function_location'] = function_in_xml[0][idx]
        code_metric_function_2.loc[dataset_tmp.index,'StartLine'] = function_in_xml[1][idx]
        code_metric_function_2.loc[dataset_tmp.index,'EndLine'] = function_in_xml[2][idx]
    elif dataset_tmp.shape[0] == 0:
        print("Not found, rows have {}, {}, and {}".format(py_file_name, function_in_xml[3][idx], function_in_xml[4][idx]))
    else:
        print("Multiple rows have {}, {}, and {}".format(py_file_name, function_in_xml[3][idx], function_in_xml[4][idx]))

# check how many rows not changed
print()
print(Counter(list(code_metric_function_2['function_location'])))


for idx in range(len(function_in_xml[4])):
    py_path = function_in_xml[0][idx]
    py_file_name = py_path[get_last_index(py_path,'/')+1:py_path.index(".py")]
    func_1_name = function_in_xml[3][idx]
    # print(func_1_name)
    func_1_name = func_1_name[:func_1_name.index('(')] if '(' in func_1_name else func_1_name
    func_2_name = function_in_xml[4][idx].replace('(','\(').replace(')','\)').replace('*','\*')
    
    # print(repr(py_file_name), repr(func_1_name), repr(func_2_name))
    
    # look for the dataframe rows that contain both py_file_name and func_name
    dataset_tmp = code_metric_function_2[(code_metric_function_2['Name'].str.contains(py_file_name)) & 
                                         (code_metric_function_2['Name']==func_1_name) &
                                         (code_metric_function_2['Name']==func_2_name) &
                                         (code_metric_function_2['function_location']=='not_assigned')]
    # print(dataset_tmp.shape[0])
    if dataset_tmp.shape[0] == 1: 
        code_metric_function_2.loc[dataset_tmp.index,'function_location'] = function_in_xml[0][idx]
        code_metric_function_2.loc[dataset_tmp.index,'StartLine'] = function_in_xml[1][idx]
        code_metric_function_2.loc[dataset_tmp.index,'EndLine'] = function_in_xml[2][idx]
    elif dataset_tmp.shape[0] == 0:
        print("Not found, rows have {}, {}, and {}".format(py_file_name, function_in_xml[3][idx], function_in_xml[4][idx]))
    else:
        print("Multiple rows have {}, {}, and {}".format(py_file_name, function_in_xml[3][idx], function_in_xml[4][idx]))

# check how many rows not changed
print()
print(Counter(list(code_metric_function_2['function_location'])))

code_metric_function_2.head()

# case 3 - use the py files and xml file to check the remaining functions

dataset_tmp = code_metric_function_2[code_metric_function_2['function_location']=='not_assigned']['Name']
func_to_check = list(dataset_tmp)
print("The functions that need to further check: {}\n{}".format(len(func_to_check),func_to_check[0:5]))
print()
func_to_check = [_[get_last_index(_,'.')+1:] for _ in func_to_check]
print("The names of the functions to check: {}\n{}".format(len(func_to_check),func_to_check[0:5]))
print()
func_in_xml_lines = {}
for idx_func in range(len(func_to_check)):
    if "," in func_to_check[idx_func]:
        _ = func_to_check[idx_func]
        _ = _.replace('(',',').replace(')',',')
        list_tmp = _.split(',')
        # print(list_tmp)
        func_in_xml_lines[func_to_check[idx_func]]=[]
        f = open(function_xml_path, "r", encoding='utf-8')
        lines = f.readlines()
        f.close()
        for idx_ln in range(len(lines)):
            if "def" in lines[idx_ln] and all_list_string_in_one_string_line(lines[idx_ln], list_tmp):
                # print("{} in line {}: {}".format(list_tmp, idx_ln, lines[idx_ln]))
                func_in_xml_lines[func_to_check[idx_func]].append(idx_ln)           
    else:
        # print(func_to_check[idx_func])
        func_in_xml_lines[func_to_check[idx_func]]=[]
        f = open(function_xml_path, "r", encoding='utf-8')
        lines = f.readlines()
        f.close()
        for idx_ln in range(len(lines)):
            if "def" in lines[idx_ln] and func_to_check[idx_func] in lines[idx_ln]:
                # print("{} in line {}: {}".format(func_to_check[idx_func], idx_ln, lines[idx_ln]))
                func_in_xml_lines[func_to_check[idx_func]].append(idx_ln)

function_to_verify_by_py_and_xml = []
for func in list(dataset_tmp):
    func_ = func[get_last_index(func,'.')+1:]
    if len(func_in_xml_lines[func_]) != 0:
        # print("{}:{}".format(func,func_in_xml_lines[func_]))
        function_to_verify_by_py_and_xml.append(func)
        # open the .py files in directories, obtain the startline and then compare to that in the .xml file
print("The functions that might be further parsed: {}\n{}".format(len(function_to_verify_by_py_and_xml), function_to_verify_by_py_and_xml[0:5]))
print()
        
# obtain all the .py files in the directory
# path_to_walk = "/content/drive/MyDrive/Software_data/code_metrics/py/django-main"
path_to_walk = MAIN_PATH + "code_metrics/py/django-main"

path_file = []

for root, dirs, files in os.walk(path_to_walk, topdown=False):
    for _ in files:
        path_file.append(os.path.join(root, _))

py_file = [_ for _ in path_file if ".py" in _]
print("{} .py files in total".format(len(py_file)))
print("The first 5 .py files:\n{}".format(py_file[0:5]))

for func in function_to_verify_by_py_and_xml:
    # seprate the path, function name, and arguments
    # idx = get_last_index(func,".")
    # func_path = func[:idx].split(".")
    str1 = func
    idx = get_last_index(str1,".")
    str1 = str1[:idx]
    str1 = str1[:get_first_uppercase_index(str1)] if get_first_uppercase_index(str1) != None else str1
    func_path = str1.replace("."," ").split()    
    # print(str1)        
    func_name = func[idx+1:][:func[idx+1:].index('(')]
    func_name = "def " + func_name 
    func_argu = func[idx+1:][func[idx+1:].index('(')+1:].replace(',',' ').replace(')',' ').split()
    
    for py in py_file:
        # if the func path seems to match the py file path, open this py file and check
        if all_list_string_in_one_string_line(py, func_path):
            f_py = open(py, "r", encoding='utf-8')
            lines_py = f_py.readlines()
            f_py.close()
            # if the function name and arguments are all in the line, retrieve the line number and read xml file to compare
            # if the startline number matches and only 1 match is found, we assume the function is found
            for idx_ln_py in range(len(lines_py)):
                if not all_list_string_in_one_string_line(lines_py[idx_ln_py], ["def","(",")",":"]):
                    continue
                if any_list_string_in_one_string_line(lines_py[idx_ln_py], ["-","{","}","@","default"]):
                    continue
                # print(lines_py[idx_ln_py])
                if func_name != lines_py[idx_ln_py][lines_py[idx_ln_py].index("def"):lines_py[idx_ln_py].index("(")]:
                    continue
                if len(func_argu) == 0 or (len(func_argu) != 0 and all_list_string_in_one_string_line(lines_py[idx_ln_py], func_argu)):
                    # retrieve the start line number in .py file
                    start_line_py = idx_ln_py + 1
                    # check the information in the xml file
                    f_xml = open(function_xml_path, "r", encoding='utf-8')
                    lines_xml = f_xml.readlines()
                    f_xml.close()
                    function_match_found = []
                    for idx_ln_xml in range(len(lines_xml)):
                        # if the function name and arguments are all in the line of xml file, retrieve the line number
                        if not all_list_string_in_one_string_line(lines_xml[idx_ln_xml], ["def","(",")",":"]):
                            continue
                        if any_list_string_in_one_string_line(lines_xml[idx_ln_xml], ["-","{","}","@","default"]):
                            continue
                        # print(lines_xml[idx_ln_xml])
                        if func_name != lines_xml[idx_ln_xml][lines_xml[idx_ln_xml].index("def"):lines_xml[idx_ln_xml].index("(")]:
                            continue
                        if (len(func_argu) == 0 or (len(func_argu) != 0 \
                            and all_list_string_in_one_string_line(lines_xml[idx_ln_xml], func_argu))) \
                            and "<source file=" in lines_xml[idx_ln_xml-1]:
                            ln_xml_func = lines_xml[idx_ln_xml-1]
                            # print(ln_xml_func)
                            ln_xml_func = ln_xml_func[ln_xml_func.index('startline="')+len('startline="'):]
                            start_line_xml = int(ln_xml_func[:ln_xml_func.index('"')])
                            if start_line_xml == start_line_py:
                                ln_xml_func = lines_xml[idx_ln_xml-1]
                                ln_xml_func = ln_xml_func[ln_xml_func.index('django-main'):]
                                func_path_xml = ln_xml_func[:ln_xml_func.index('"')]
                                ln_xml_func = lines_xml[idx_ln_xml-1]
                                ln_xml_func = ln_xml_func[ln_xml_func.index('endline="')+len('endline="'):]
                                end_line_xml = int(ln_xml_func[:ln_xml_func.index('"')])
                                function_match_found.append([func_path_xml, start_line_xml, end_line_xml])
                    # after traverse each line in the xml file, decide whether the result is unique
                    # if yes, assign it to the dataframe
                    if len(function_match_found)==1:
                        print("Match found for {} at line {} in {} and {}".format(func, start_line_py, py, lines_xml[idx_ln_xml-1]))
                        # assign values to dataframe
                        # look for the dataframe rows that contains the specific function name and has not been assigned
                        dataset_tmp = code_metric_function_2[(code_metric_function_2['Name']==func) &
                                                             (code_metric_function_2['function_location']=='not_assigned')]
                        if dataset_tmp.shape[0] == 1: 
                            code_metric_function_2.loc[dataset_tmp.index,'function_location'] = function_match_found[0][0]
                            code_metric_function_2.loc[dataset_tmp.index,'StartLine'] = function_match_found[0][1]
                            code_metric_function_2.loc[dataset_tmp.index,'EndLine'] = function_match_found[0][2]
                        elif dataset_tmp.shape[0] == 0:
                            print("Not found, rows have {}".format(func))
                        else:
                            print("Multiple rows have {}".format(func))    
                            
                    elif len(function_match_found)>1:
                        print("Multiple matches found for {} at {}".format(func, function_match_found))
                    else:
                        print("No matches found for {}".format(func))
                            
# check how many rows not changed
print()
print(Counter(list(code_metric_function_2['function_location'])))

code_metric_function_2.head()

# save and load dataframe
code_metric_function_2_path = "/content/drive/MyDrive/Software_data/code_metrics/code_metric_function_2.pkl"
# code_metric_function_2_path = "/content/drive/MyDrive/Software_data/code_metrics/code_metric_function_django.pkl"

# code_metric_function_2.to_pickle(code_metric_function_2_path) 

code_metric_function_2 = pd.read_pickle(code_metric_function_2_path)

code_metric_function_2

django_dataset = code_metric_function_2[code_metric_function_2['function_location']!='not_assigned']

print("There are {} instances.".format(django_dataset.shape[0]))

# django_dataset.head()

django_dataset['is_cloned']=[0 for _ in range(django_dataset.shape[0])]
django_dataset = django_dataset[['function_location',
                                                 'Name',
                                                 'StartLine',
                                                 'EndLine',
                                                 'CountLine', 
                                                 'CountLineBlank', 
                                                 'CountLineCode', 
                                                 'CountLineCodeDecl', 
                                                 'CountLineCodeExe', 
                                                 'CountLineComment', 
                                                 'CountPath', 
                                                 'CountPathLog', 
                                                 'CountStmt', 
                                                 'CountStmtDecl', 
                                                 'CountStmtExe', 
                                                 'Cyclomatic', 
                                                 'CyclomaticModified', 
                                                 'CyclomaticStrict', 
                                                 'Essential', 
                                                 'MaxNesting', 
                                                 'RatioCommentToCode',
                                                 'is_cloned']]
django_dataset.head()

# parse the functions.xml file to locate the functions
# function_clone_xml_path = ["/content/drive/MyDrive/Software_data/code_metrics/xml/django/type1_django-main_functions-clones/django-main_functions-clones-0.00.xml",
#                            "/content/drive/MyDrive/Software_data/code_metrics/xml/django/type2_django-main_functions-blind-clones/django-main_functions-blind-clones-0.00.xml",
#                            "/content/drive/MyDrive/Software_data/code_metrics/xml/django/type2c_django-main_functions-consistent-clones/django-main_functions-consistent-clones-0.00.xml",
#                            "/content/drive/MyDrive/Software_data/code_metrics/xml/django/type3-1_django-main_functions-clones/django-main_functions-clones-0.30.xml",
#                            "/content/drive/MyDrive/Software_data/code_metrics/xml/django/type3-2_django-main_functions-blind-clones/django-main_functions-blind-clones-0.30.xml",
#                            "/content/drive/MyDrive/Software_data/code_metrics/xml/django/type3-2c_django-main_functions-consistent-clones/django-main_functions-consistent-clones-0.30.xml"]

function_clone_xml_path = [MAIN_PATH+"code_metrics/xml/django/type1_django-main_functions-clones/django-main_functions-clones-0.00.xml",
                           MAIN_PATH+"code_metrics/xml/django/type2_django-main_functions-blind-clones/django-main_functions-blind-clones-0.00.xml",
                           MAIN_PATH+"code_metrics/xml/django/type2c_django-main_functions-consistent-clones/django-main_functions-consistent-clones-0.00.xml",
                           MAIN_PATH+"code_metrics/xml/django/type3-1_django-main_functions-clones/django-main_functions-clones-0.30.xml",
                           MAIN_PATH+"code_metrics/xml/django/type3-2_django-main_functions-blind-clones/django-main_functions-blind-clones-0.30.xml",
                           MAIN_PATH+"code_metrics/xml/django/type3-2c_django-main_functions-consistent-clones/django-main_functions-consistent-clones-0.30.xml"]

for xml_path in function_clone_xml_path:
    # read lines of each xml file
    f = open(xml_path, "r", encoding='utf-8')
    lines = f.readlines()
    f.close()
    
    # parse the line and assign the label to corresponding row
    for idx in range(len(lines)):
        ln = lines[idx]
        if "<source file=" in ln:
            ln = ln[ln.index("django-main"):]
            func_loc = ln[:ln.index('"')]
            ln = ln[ln.index('startline="')+len('startline="'):]
            start_line = int(ln[:ln.index('"')])
            ln = ln[ln.index('endline="')+len('endline="'):]
            end_line = int(ln[:ln.index('"')])
            # print(func_loc, start_line, end_line)
            
            # update the dataframe
            # look for the dataframe rows that contain both py_file_name and func_name
            dataset_tmp = django_dataset[(django_dataset['function_location']==func_loc) &
                                        (django_dataset['StartLine']==start_line) &
                                        (django_dataset['EndLine']==end_line) &
                                        (django_dataset['is_cloned']==0)]
            
            if dataset_tmp.shape[0] == 1:
                django_dataset.loc[dataset_tmp.index,'is_cloned'] = 1
    print("Parsing cloned functions is completed for {}.".format(xml_path))

# check how many rows not changed
print()
print(Counter(list(django_dataset['is_cloned'])))

django_dataset.head()

dataset_1 = django_dataset[['CountLine', 
                             'CountLineBlank', 
                             'CountLineCode', 
                             'CountLineCodeDecl', 
                             'CountLineCodeExe', 
                             'CountLineComment', 
                             'CountPath', 
                             'CountPathLog', 
                             'CountStmt', 
                             'CountStmtDecl', 
                             'CountStmtExe', 
                             'Cyclomatic', 
                             'CyclomaticModified', 
                             'CyclomaticStrict', 
                             'Essential', 
                             'MaxNesting', 
                             'RatioCommentToCode',
                             'is_cloned']]
dataset_1.head()

# count the values occurrence in each column
for _ in dataset_1.columns.tolist():
  print(_)
  print(Counter(dataset_1[_]))

# save and load dataframe
dataframe_django_path = "/content/drive/MyDrive/Software_data/code_metrics/dataframe_django.pkl"

dataset_1.to_pickle(dataframe_django_path) 

dataset_1 = pd.read_pickle(dataframe_django_path)

dataset_1 = shuffle(dataset_1)
dataset_1_X = dataset_1.iloc[:,0:dataset_1.shape[1]-1]
print("dataset_1_X has a shape of {}".format(dataset_1_X.shape))
print("the first 5 instances:\n{}".format(dataset_1_X.iloc[0:5,:]))
dataset_1_y = dataset_1.iloc[:,-1:]
print("dataset_1_y has a shape of {}".format(dataset_1_y.shape))
print("the first 5 instances:\n{}".format(dataset_1_y.iloc[0:5,:]))
print()

# split into train test sets
dataset_1_X_train, dataset_1_X_test, dataset_1_y_train, dataset_1_y_test = train_test_split(dataset_1_X, dataset_1_y, test_size=0.2, stratify = dataset_1_y)
np.save(MAIN_PATH+"dataset/django_X_train.npy",np.array(dataset_1_X_train))
np.save(MAIN_PATH+"dataset/django_X_test.npy",np.array(dataset_1_X_test))
np.save(MAIN_PATH+"dataset/django_y_train.npy",np.array(dataset_1_y_train).flatten())
np.save(MAIN_PATH+"dataset/django_y_test.npy",np.array(dataset_1_y_test).flatten())
# dataset_1_X_train_back = dataset_1_X_train.copy()
# dataset_1_X_test_back = dataset_1_X_test.copy()

# verify the class distribution
print("class 0:1 in dataset_1_y is       : {}:{}".format(Counter(list(dataset_1_y['is_cloned']))[0]/dataset_1_y.shape[0], 1-Counter(list(dataset_1_y['is_cloned']))[0]/dataset_1_y.shape[0]))
print("class 0:1 in dataset_1_y_train is : {}:{}".format(Counter(list(dataset_1_y_train['is_cloned']))[0]/dataset_1_y_train.shape[0], 1-Counter(list(dataset_1_y_train['is_cloned']))[0]/dataset_1_y_train.shape[0]))
print("class 0:1 in dataset_1_y_test is  : {}:{}".format(Counter(list(dataset_1_y_test['is_cloned']))[0]/dataset_1_y_test.shape[0], 1-Counter(list(dataset_1_y_test['is_cloned']))[0]/dataset_1_y_test.shape[0]))
print()

# the train set will be further split by stratified splitting for cross validation
skf = StratifiedKFold(n_splits=5)
skf_tr_va_sets = skf.split(dataset_1_X_train, dataset_1_y_train)
for idx_f, (idx_tr, idx_va) in enumerate(skf_tr_va_sets):
  tr_X = dataset_1_X.iloc[idx_tr]
  tr_y = dataset_1_y.iloc[idx_tr]
  va_X = dataset_1_X.iloc[idx_va]
  va_y = dataset_1_y.iloc[idx_va]
  print("Fold {}\ntrain set X:{} train set y: {} validation set X:{} validation set y: {}".format(idx_f,tr_X.shape,tr_y.shape,va_X.shape,va_y.shape))
  print("class 0:1 in train set y is      : {}:{}".format(Counter(list(tr_y['is_cloned']))[0]/tr_y.shape[0], 1-Counter(list(tr_y['is_cloned']))[0]/tr_y.shape[0]))
  print("class 0:1 in validation set y is : {}:{}".format(Counter(list(va_y['is_cloned']))[0]/va_y.shape[0], 1-Counter(list(va_y['is_cloned']))[0]/va_y.shape[0]))

"""## 4.2 Train and Test - Original Dataset

### 4.2.1 DT
"""

save_path = MAIN_PATH + "model/"
data_type = "django"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='dt')

"""### 4.2.2 RF"""

save_path = MAIN_PATH + "model/"
data_type = "django"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='rf')

"""### 4.2.3 KNN"""

save_path = MAIN_PATH + "model/"
data_type = "django"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='knn')

"""### 4.2.4 NB"""

save_path = MAIN_PATH + "model/"
data_type = "django"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nb')

"""### 4.2.5 LR"""

save_path = MAIN_PATH + "model/"
data_type = "django"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lr')

"""### 4.2.6 LDA"""

save_path = MAIN_PATH + "model/"
data_type = "django"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lda')

"""### 4.2.7 QDA"""

save_path = MAIN_PATH + "model/"
data_type = "django"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='qda')

"""### 4.2.8 SVM"""

save_path = MAIN_PATH + "model/"
data_type = "django"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='svm')

"""### 4.2.9 MLP"""

save_path = MAIN_PATH + "model/"
data_type = "django"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='mlp')

"""### 4.2.10 NN"""

save_path = MAIN_PATH + "model/"
data_type = "django"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nn')

"""## 4.3 Feature Selection

by Accuracy
"""

# use RF to select features
data_type = "django"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")

model = RandomForestClassifier(max_depth=None, random_state=0)
feature_percentage = 1.0

# Sequential Forward Selection by accuracy
sfs_acc = SFS(model, 
          k_features=int(X_train.shape[1]*feature_percentage),   # select 80% feature columns
          forward=True, 
          floating=False, 
          verbose=2,
          scoring='accuracy',
          cv=5,                                 # cross validation
          n_jobs=-1)                            # run CV on all CPU cores

sfs_acc = sfs_acc.fit(X_train, y_train)

"""by F1-score"""

# use RF to select features
data_type = "django"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")

model = RandomForestClassifier(max_depth=None, random_state=0)
feature_percentage = 1.0

# Sequential Forward Selection by accuracy
sfs_f1 = SFS(model, 
          k_features=int(X_train.shape[1]*feature_percentage),   # select 80% feature columns
          forward=True, 
          floating=False, 
          verbose=2,
          scoring='f1',
          cv=5,                                 # cross validation
          n_jobs=-1)                            # run CV on all CPU cores

sfs_f1 = sfs_f1.fit(X_train, y_train)

# select feature that increase average cv score by accuracy
print('Sequential Forward Selection (k={}) by Accuracy'.format(int(X_train.shape[1]*feature_percentage)))
print("Indices of selected features: {}".format(sfs_acc.k_feature_idx_))
print('CV Score                    : {}'.format(sfs_acc.k_score_))

data_type = "django"
feature_name = pd.read_pickle(MAIN_PATH + "code_metrics/dataframe_"+data_type+".pkl").columns.tolist()
feature_name = feature_name[:-1]

feature_name_selected = [feature_name[idx] for idx in sfs_acc.k_feature_idx_]
print("Selected features           : {}".format(feature_name_selected))

feature_idx_selected_by_sequence = []
feature_cv_avg_score = []
for _ in sfs_acc.subsets_:
  # print(sfs_acc.subsets_[_])
  for idx in sfs_acc.subsets_[_]['feature_names']:
    if int(idx) not in feature_idx_selected_by_sequence:
      feature_idx_selected_by_sequence.append(int(idx))
      feature_cv_avg_score.append(sfs_acc.subsets_[_]['avg_score'])

print("Feature indices selected by sequence               : {}".format(feature_idx_selected_by_sequence))
print("CV average score by current feature selection      : {}".format(feature_cv_avg_score))

# select only the features keep increasing the CV average score
target_idx = 0
for idx in range(len(feature_cv_avg_score)):
  if idx+1 == len(feature_cv_avg_score) or feature_cv_avg_score[idx+1] <= feature_cv_avg_score[idx]:
    target_idx = idx
    break

feature_idx_increase_cv_score =  feature_idx_selected_by_sequence[:idx+1]
print("The feature indices that increase CV average score : {}".format(feature_idx_increase_cv_score))
feature_name_increase_cv_score = [feature_name[idx] for idx in feature_idx_increase_cv_score]
print("The feature names that increase CV average score   : {}".format(feature_name_increase_cv_score))
feature_idx_selected_by_acc = feature_idx_increase_cv_score

# select feature that increase average cv score by f1-score
print()
print('Sequential Forward Selection (k={}) by F1-score'.format(int(X_train.shape[1]*feature_percentage)))
print("Indices of selected features: {}".format(sfs_f1.k_feature_idx_))
print('CV Score                    : {}'.format(sfs_f1.k_score_))

data_type = "django"
feature_name = pd.read_pickle(MAIN_PATH + "code_metrics/dataframe_"+data_type+".pkl").columns.tolist()
feature_name = feature_name[:-1]

feature_name_selected = [feature_name[idx] for idx in sfs_f1.k_feature_idx_]
print("Selected features           : {}".format(feature_name_selected))

feature_idx_selected_by_sequence = []
feature_cv_avg_score = []
for _ in sfs_f1.subsets_:
  # print(sfs_f1.subsets_[_])
  for idx in sfs_f1.subsets_[_]['feature_names']:
    if int(idx) not in feature_idx_selected_by_sequence:
      feature_idx_selected_by_sequence.append(int(idx))
      feature_cv_avg_score.append(sfs_f1.subsets_[_]['avg_score'])

print("Feature indices selected by sequence               : {}".format(feature_idx_selected_by_sequence))
print("CV average score by current feature selection      : {}".format(feature_cv_avg_score))

# select only the features keep increasing the CV average score
target_idx = 0
for idx in range(len(feature_cv_avg_score)):
  if idx+1 == len(feature_cv_avg_score) or feature_cv_avg_score[idx+1] <= feature_cv_avg_score[idx]:
    target_idx = idx
    break

feature_idx_increase_cv_score =  feature_idx_selected_by_sequence[:idx+1]
print("The feature indices that increase CV average score : {}".format(feature_idx_increase_cv_score))
feature_name_increase_cv_score = [feature_name[idx] for idx in feature_idx_increase_cv_score]
print("The feature names that increase CV average score   : {}".format(feature_name_increase_cv_score))
feature_idx_selected_by_f1 = feature_idx_increase_cv_score

# keep the intersection of two lists
feature_idx_increase_cv_score = list(set(feature_idx_selected_by_acc) & set(feature_idx_selected_by_f1))

# build the feature set using the selected features
data_type = "django"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

data_type = "django_sfs"
np.save(MAIN_PATH+"dataset/"+data_type+"_X_train.npy",X_train[:,feature_idx_increase_cv_score])
np.save(MAIN_PATH+"dataset/"+data_type+"_y_train.npy",y_train)
np.save(MAIN_PATH+"dataset/"+data_type+"_X_test.npy",X_test[:,feature_idx_increase_cv_score])
np.save(MAIN_PATH+"dataset/"+data_type+"_y_test.npy",y_test)

"""## 4.4 Train and Test - SFS Dataset

### 4.4.1 DT
"""

save_path = MAIN_PATH + "model/"
data_type = "django_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='dt')

"""### 4.4.2 RF"""

save_path = MAIN_PATH + "model/"
data_type = "django_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='rf')

"""### 4.4.3 KNN"""

save_path = MAIN_PATH + "model/"
data_type = "django_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='knn')

"""### 4.4.4 NB"""

save_path = MAIN_PATH + "model/"
data_type = "django_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nb')

"""### 4.4.5 LR"""

save_path = MAIN_PATH + "model/"
data_type = "django_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lr')

"""### 4.4.6 LDA"""

save_path = MAIN_PATH + "model/"
data_type = "django_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lda')

"""### 4.4.7 QDA"""

save_path = MAIN_PATH + "model/"
data_type = "django_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='qda')

"""### 4.4.8 SVM"""

save_path = MAIN_PATH + "model/"
data_type = "django_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='svm')

"""### 4.4.9 MLP"""

save_path = MAIN_PATH + "model/"
data_type = "django_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='mlp')

"""### 4.4.10 NN"""

save_path = MAIN_PATH + "model/"
data_type = "django_sfs"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nn')

"""## 4.5 Ensemble"""

save_path = MAIN_PATH + "model/"
data_type = "django"
metrics = os.listdir(save_path)

# select classifiers by thresholds
th_acc = 0.7
th_f1s = 0.7
clf_sel = []

print("Results:")
print("-"*80)
metrics_title = ['Accuracy','F1-score','Precision','Recall','Confusion Matrix']
print(" "*45,end="")
for m_t in metrics_title:
  print("{: >11} ".format(m_t),end="")
print()
# list label encoding classifiers
idx_clf = 0
for me in metrics:
  if "metric" in me:
    if data_type in me:    # model_le_smote_svm.pkl, true_le_smote_svm.npy, pred_le_smote_svm.npy, metric_le_smote_svm.npy
      idx_clf += 1
      clf_name = me[len("metric_"):len(me)-len(".npy")] 
      print("Classifier #{:2d}: {:20s} ".format(idx_clf, clf_name), end="")
      me_list = np.load(save_path+me)
      me_list = me_list.tolist()
      print("Metrics:",end="")
      for m_l in me_list:
        print("{: >11} ".format("{:4.7f}".format(m_l)),end="")
      print()

      # select the classifier if it meets thresholds
      if me_list[0]>=th_acc and me_list[1]>=th_f1s:
        clf_sel.append(clf_name)

print("{} classifiers selected:{}\n".format(len(clf_sel),clf_sel))
if len(clf_sel) > 0:
  # output the true and predictions of each selected classifier
  for i in range(len(clf_sel)):
    print("classifier #{:2d}".format(i+1))
    print("True      :",np.load(save_path+"true_"+clf_sel[i]+".npy").astype(np.uint8).tolist())
    print("Prediction:",np.load(save_path+"pred_"+clf_sel[i]+".npy").astype(np.uint8).tolist())

  # check whether the saved true values are the same as the original values
  flask_y_test = np.load(MAIN_PATH+"dataset/"+"flask_y_test.npy")

  # check whether each saved y values match the label encoding y values
  for i in range(len(clf_sel)):
    true = np.load(save_path+"true_"+clf_sel[i]+".npy")
    for j in range(len(true)):
      if list(flask_y_test)[j] - true.flatten()[j] !=0:
        print("y values of label encoding does not match those of saved y values for {}".format(clf_sel[i]))
        break
      elif j == len(true)-1:
        print("y values of label encoding match those of saved y vaues for {}".format(clf_sel[i]))  
      
  # generate the prediction values by voting
  pred = []
  for i in range(len(clf_sel)):
    pred.append(np.load(save_path+"pred_"+clf_sel[i]+".npy").astype(np.uint8).tolist())
  pred = np.array(pred)
  pred_ensemble = []
  for i in range(pred.shape[1]):
    if sum(pred[:,i])>=pred.shape[0]-sum(pred[:,i]):
      pred_ensemble.append(1)
    else:
      pred_ensemble.append(0)
  true = np.load(save_path+"true_"+clf_sel[0]+".npy").astype(np.uint8).tolist()
  print("True                :",true)
  print("Prediction by voting:",pred_ensemble)

  print("Accuracy score             :",accuracy_score(true, pred_ensemble))
  print("F1-score                   :",f1_score(true, pred_ensemble))
  print("Precision score            :",precision_score(true, pred_ensemble))
  print("Recall score               :",recall_score(true, pred_ensemble))
  print("Classification report      :\n",classification_report(true, pred_ensemble))
  print("Confusion Matrix           :\n",confusion_matrix(true, pred_ensemble))

  # plot confusion matrix
  cm = confusion_matrix(true, pred_ensemble)
  plot_confusion_matrix(cm, normalize = True, target_names = ["not readmitted","readmitted"], title = "Confusion Matrix")

else:
  print("No reliable results!")

"""## 4.6 Visualize the histogram distribution of "not cloned" and "cloned" instances"""

# load dataframe
dataframe_django_path = "/content/drive/MyDrive/Software_data/code_metrics/dataframe_django.pkl"
dataframe_django = pd.read_pickle(dataframe_django_path)

for _ in dataframe_django.columns.tolist():
  print(_)
  print(Counter(dataframe_django[_]))

dataframe_django.head()

dataframe_django_cloned = dataframe_django[dataframe_django['is_cloned']==1]
dataframe_django_cloned

dataframe_django_non_cloned = dataframe_django[dataframe_django['is_cloned']==0]
dataframe_django_non_cloned

dataframe_django_cloned['RatioCommentToCode'] = dataframe_django_cloned['RatioCommentToCode'] * 100
dataframe_django_cloned = dataframe_django_cloned.astype(int)
dataframe_django_non_cloned['RatioCommentToCode'] = dataframe_django_non_cloned['RatioCommentToCode'] * 100
dataframe_django_non_cloned = dataframe_django_non_cloned.astype(int)

# observe how the bins number influence the distribution
feature_names = dataframe_django_cloned.columns.tolist()[:-1]
for f_n in feature_names: 
    print("cloned -> feature: {} min: {} max: {}".format(f_n,dataframe_django_cloned[f_n].min(),dataframe_django_cloned[f_n].max()))
    print("{}".format(sorted(Counter(dataframe_django_cloned[f_n]).items())))
    
    y_value = dataframe_django_cloned[f_n]
    x_value = range(dataframe_django_cloned[f_n].min(),dataframe_django_cloned[f_n].max()+1,int((dataframe_django_cloned[f_n].max()+1-dataframe_django_cloned[f_n].min())/10)+1)
    
    fig, axes = plt.subplots(nrows=1, ncols=8, figsize=(25,2))
    
    for _ in range(2,10):
        axes[_-2].hist(y_value,bins=_)
        axes[_-2].set_title("bins="+str(_))
        axes[_-2].set_xticks(x_value)
        axes[_-2].set_xticklabels(x_value,fontsize=8,rotation=90)
        
    plt.show()

    print("non-cloned -> feature: {} min: {} max: {}".format(f_n,dataframe_django_non_cloned[f_n].min(),dataframe_django_non_cloned[f_n].max()))
    print("{}".format(sorted(Counter(dataframe_django_non_cloned[f_n]).items())))
    
    y_value = dataframe_django_non_cloned[f_n]
    x_value = range(dataframe_django_non_cloned[f_n].min(),dataframe_django_non_cloned[f_n].max()+1,int((dataframe_django_non_cloned[f_n].max()+1-dataframe_django_non_cloned[f_n].min())/10)+1)
    
    fig, axes = plt.subplots(nrows=1, ncols=8, figsize=(25,2))
    
    for _ in range(2,10):
        axes[_-2].hist(y_value,bins=_,color='green')
        axes[_-2].set_title("bins="+str(_))
        axes[_-2].set_xticks(x_value)
        axes[_-2].set_xticklabels(x_value,fontsize=8,rotation=90)
        
    plt.show()

# find the threshold to differentiate the instances
thres = {}
feature_names = dataframe_django_cloned.columns.tolist()[:-1]
for f_n in feature_names: 
    print(f_n)
    x1 = dataframe_django_cloned[f_n]
    y1 = dataframe_django_cloned['is_cloned']
    print("Cloned     : min={}, max={}".format(x1.min(),x1.max()))
    plt.figure(figsize=(25, 2))
    plt.scatter(x1,y1)
    x2 = dataframe_django_non_cloned[f_n]
    y2 = dataframe_django_non_cloned['is_cloned']
    print("non-Cloned : min={}, max={}".format(x2.min(),x2.max()))
    plt.scatter(x2,y2,color='green')
    thres[f_n]= x1.max()+1 if x1.max()<=x2.max() else x2.max()+1
    print("threshold  : {}".format(thres[f_n]))
    plt.show()

"""## 4.7 Create new dataset - by Strategy 1: threshold value"""

feature_names = dataframe_django.columns.tolist()[:-1]
dataframe_django_new = dataframe_django.copy()
for f_n in feature_names: 
  dataframe_django_new[f_n+"_larger_than_threshold"] = dataframe_django_new[f_n]>thres[f_n]
  dataframe_django_new[f_n+"_larger_than_threshold"] = dataframe_django_new[f_n+"_larger_than_threshold"].astype(int)
feature_names = dataframe_django_new.columns.tolist()
feature_names.remove("is_cloned")
feature_names = feature_names + ['is_cloned']
print("Feature names: {}".format(feature_names))
dataframe_django_new = dataframe_django_new[feature_names]
dataframe_django_new.head()

"""## 4.8 Split and save new train and test sets"""

# count the values occurrence in each column
for _ in dataframe_django_new.columns.tolist():
  print(_)
  print(Counter(dataframe_django_new[_]))

# save and load dataframe
dataframe_django_new_path = "/content/drive/MyDrive/Software_data/code_metrics/dataframe_django_new.pkl"

dataframe_django_new.to_pickle(dataframe_django_new_path) 

dataset_1 = pd.read_pickle(dataframe_django_new_path)

dataset_1 = shuffle(dataset_1)
dataset_1_new_X = dataset_1.iloc[:,0:dataset_1.shape[1]-1]
print("dataset_1_new_X has a shape of {}".format(dataset_1_new_X.shape))
print("the first 5 instances:\n{}".format(dataset_1_new_X.iloc[0:5,:]))
dataset_1_new_y = dataset_1.iloc[:,-1:]
print("dataset_1_new_y has a shape of {}".format(dataset_1_new_y.shape))
print("the first 5 instances:\n{}".format(dataset_1_new_y.iloc[0:5,:]))
print()

# split into train test sets
dataset_1_new_X_train, dataset_1_new_X_test, dataset_1_new_y_train, dataset_1_new_y_test = train_test_split(dataset_1_new_X, dataset_1_new_y, test_size=0.2, stratify = dataset_1_new_y)
np.save(MAIN_PATH+"dataset/django_new_X_train.npy",np.array(dataset_1_new_X_train))
np.save(MAIN_PATH+"dataset/django_new_X_test.npy",np.array(dataset_1_new_X_test))
np.save(MAIN_PATH+"dataset/django_new_y_train.npy",np.array(dataset_1_new_y_train).flatten())
np.save(MAIN_PATH+"dataset/django_new_y_test.npy",np.array(dataset_1_new_y_test).flatten())
# dataset_1_new_X_train_back = dataset_1_new_X_train.copy()
# dataset_1_new_X_test_back = dataset_1_new_X_test.copy()

# verify the class distribution
print("class 0:1 in dataset_1_new_y is       : {}:{}".format(Counter(list(dataset_1_new_y['is_cloned']))[0]/dataset_1_new_y.shape[0], 1-Counter(list(dataset_1_new_y['is_cloned']))[0]/dataset_1_new_y.shape[0]))
print("class 0:1 in dataset_1_new_y_train is : {}:{}".format(Counter(list(dataset_1_new_y_train['is_cloned']))[0]/dataset_1_new_y_train.shape[0], 1-Counter(list(dataset_1_new_y_train['is_cloned']))[0]/dataset_1_new_y_train.shape[0]))
print("class 0:1 in dataset_1_new_y_test is  : {}:{}".format(Counter(list(dataset_1_new_y_test['is_cloned']))[0]/dataset_1_new_y_test.shape[0], 1-Counter(list(dataset_1_new_y_test['is_cloned']))[0]/dataset_1_new_y_test.shape[0]))
print()

# the train set will be further split by stratified splitting for cross validation
skf = StratifiedKFold(n_splits=5)
skf_tr_va_sets = skf.split(dataset_1_new_X_train, dataset_1_new_y_train)
for idx_f, (idx_tr, idx_va) in enumerate(skf_tr_va_sets):
  tr_X = dataset_1_new_X.iloc[idx_tr]
  tr_y = dataset_1_new_y.iloc[idx_tr]
  va_X = dataset_1_new_X.iloc[idx_va]
  va_y = dataset_1_new_y.iloc[idx_va]
  print("Fold {}\ntrain set X:{} train set y: {} validation set X:{} validation set y: {}".format(idx_f,tr_X.shape,tr_y.shape,va_X.shape,va_y.shape))
  print("class 0:1 in train set y is      : {}:{}".format(Counter(list(tr_y['is_cloned']))[0]/tr_y.shape[0], 1-Counter(list(tr_y['is_cloned']))[0]/tr_y.shape[0]))
  print("class 0:1 in validation set y is : {}:{}".format(Counter(list(va_y['is_cloned']))[0]/va_y.shape[0], 1-Counter(list(va_y['is_cloned']))[0]/va_y.shape[0]))

"""## 4.9 train and test

### 4.9.1 DT
"""

save_path = MAIN_PATH + "model/"
data_type = "django_new"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='dt')

"""### 4.9.2 RF"""

save_path = MAIN_PATH + "model/"
data_type = "django_new"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='rf')

"""### 4.9.3 KNN"""

save_path = MAIN_PATH + "model/"
data_type = "django_new"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='knn')

"""### 4.9.4 NB"""

save_path = MAIN_PATH + "model/"
data_type = "django_new"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nb')

"""### 4.9.5 LR"""

save_path = MAIN_PATH + "model/"
data_type = "django_new"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lr')

"""### 4.9.6 LDA"""

save_path = MAIN_PATH + "model/"
data_type = "django_new"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lda')

"""### 4.9.7 QDA"""

save_path = MAIN_PATH + "model/"
data_type = "django_new"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='qda')

"""### 4.9.8 SVM"""

save_path = MAIN_PATH + "model/"
data_type = "django_new"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='svm')

"""### 4.9.9 MLP"""

save_path = MAIN_PATH + "model/"
data_type = "django_new"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='mlp')

"""### 4.9.10 NN"""

save_path = MAIN_PATH + "model/"
data_type = "django_new"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nn')

"""## 4.10 Ensemble"""

save_path = MAIN_PATH + "model/"
data_type = "django"
metrics = os.listdir(save_path)

# select classifiers by thresholds
th_acc = 0.7
th_f1s = 0.7
clf_sel = []

print("Results:")
print("-"*80)
metrics_title = ['Accuracy','F1-score','Precision','Recall','Confusion Matrix']
print(" "*45,end="")
for m_t in metrics_title:
  print("{: >11} ".format(m_t),end="")
print()
# list label encoding classifiers
idx_clf = 0
for me in metrics:
  if "metric" in me:
    if data_type in me:    # model_le_smote_svm.pkl, true_le_smote_svm.npy, pred_le_smote_svm.npy, metric_le_smote_svm.npy
      idx_clf += 1
      clf_name = me[len("metric_"):len(me)-len(".npy")] 
      print("Classifier #{:2d}: {:20s} ".format(idx_clf, clf_name), end="")
      me_list = np.load(save_path+me)
      me_list = me_list.tolist()
      print("Metrics:",end="")
      for m_l in me_list:
        print("{: >11} ".format("{:4.7f}".format(m_l)),end="")
      print()

      # select the classifier if it meets thresholds
      if me_list[0]>=th_acc and me_list[1]>=th_f1s:
        clf_sel.append(clf_name)

print("{} classifiers selected:{}\n".format(len(clf_sel),clf_sel))
if len(clf_sel) > 0:
  # output the true and predictions of each selected classifier
  for i in range(len(clf_sel)):
    print("classifier #{:2d}".format(i+1))
    print("True      :",np.load(save_path+"true_"+clf_sel[i]+".npy").astype(np.uint8).tolist())
    print("Prediction:",np.load(save_path+"pred_"+clf_sel[i]+".npy").astype(np.uint8).tolist())

  # check whether the saved true values are the same as the original values
  flask_y_test = np.load(MAIN_PATH+"dataset/"+"flask_y_test.npy")

  # check whether each saved y values match the label encoding y values
  for i in range(len(clf_sel)):
    true = np.load(save_path+"true_"+clf_sel[i]+".npy")
    for j in range(len(true)):
      if list(flask_y_test)[j] - true.flatten()[j] !=0:
        print("y values of label encoding does not match those of saved y values for {}".format(clf_sel[i]))
        break
      elif j == len(true)-1:
        print("y values of label encoding match those of saved y vaues for {}".format(clf_sel[i]))  
      
  # generate the prediction values by voting
  pred = []
  for i in range(len(clf_sel)):
    pred.append(np.load(save_path+"pred_"+clf_sel[i]+".npy").astype(np.uint8).tolist())
  pred = np.array(pred)
  pred_ensemble = []
  for i in range(pred.shape[1]):
    if sum(pred[:,i])>=pred.shape[0]-sum(pred[:,i]):
      pred_ensemble.append(1)
    else:
      pred_ensemble.append(0)
  true = np.load(save_path+"true_"+clf_sel[0]+".npy").astype(np.uint8).tolist()
  print("True                :",true)
  print("Prediction by voting:",pred_ensemble)

  print("Accuracy score             :",accuracy_score(true, pred_ensemble))
  print("F1-score                   :",f1_score(true, pred_ensemble))
  print("Precision score            :",precision_score(true, pred_ensemble))
  print("Recall score               :",recall_score(true, pred_ensemble))
  print("Classification report      :\n",classification_report(true, pred_ensemble))
  print("Confusion Matrix           :\n",confusion_matrix(true, pred_ensemble))

  # plot confusion matrix
  cm = confusion_matrix(true, pred_ensemble)
  plot_confusion_matrix(cm, normalize = True, target_names = ["not readmitted","readmitted"], title = "Confusion Matrix")

else:
  print("No reliable results!")

"""## 4.11 Create new dataset - by Strategy 2: similarity with vectors of the mean and median of each dimension """

# load dataframe
dataframe_django_path = "/content/drive/MyDrive/Software_data/code_metrics/dataframe_django.pkl"
dataframe_django = pd.read_pickle(dataframe_django_path)

dataframe_django.head()

# calculate the vectors of the mean and median of each dimension
dataframe_django_sim = dataframe_django.iloc[:,:-1].copy()
dataframe_django_sim_mean = dataframe_django_sim.mean(axis=0)
dataframe_django_sim_median = dataframe_django_sim.median(axis=0)

# calculate the cosine similarity of each row with the mean and median vectors
df_cos_sim_mean = dataframe_django_sim.apply(lambda row: cosine_similarity(np.array(row).reshape(1,-1), np.array(dataframe_django_sim_mean).reshape(1,-1)), axis=1)
df_cos_sim_mean = df_cos_sim_mean.tolist()
df_cos_sim_mean = [_.flatten()[0] for _ in df_cos_sim_mean]

df_cos_sim_median = dataframe_django_sim.apply(lambda row: cosine_similarity(np.array(row).reshape(1,-1), np.array(dataframe_django_sim_median).reshape(1,-1)), axis=1)
df_cos_sim_median = df_cos_sim_median.tolist()
df_cos_sim_median = [_.flatten()[0] for _ in df_cos_sim_median]

dataframe_django_sim['cosine_similarity_with_mean'] = df_cos_sim_mean
dataframe_django_sim['cosine_similarity_with_median'] = df_cos_sim_median

dataframe_django_sim['is_cloned'] = dataframe_django.iloc[:,-1:].copy()
dataframe_django_sim.head()

"""## 4.12 Split and save new train and test sets"""

# count the values occurrence in each column
for _ in dataframe_django_sim.columns.tolist():
  print(_)
  print(Counter(dataframe_django_sim[_]))

# save and load dataframe
dataframe_django_sim_path = "/content/drive/MyDrive/Software_data/code_metrics/dataframe_django_sim.pkl"

dataframe_django_sim.to_pickle(dataframe_django_sim_path) 

dataset_1 = pd.read_pickle(dataframe_django_sim_path)

dataset_1 = shuffle(dataset_1)
dataset_1_sim_X = dataset_1.iloc[:,0:dataset_1.shape[1]-1]
print("dataset_1_sim_X has a shape of {}".format(dataset_1_sim_X.shape))
print("the first 5 instances:\n{}".format(dataset_1_sim_X.iloc[0:5,:]))
dataset_1_sim_y = dataset_1.iloc[:,-1:]
print("dataset_1_sim_y has a shape of {}".format(dataset_1_sim_y.shape))
print("the first 5 instances:\n{}".format(dataset_1_sim_y.iloc[0:5,:]))
print()

# split into train test sets
dataset_1_sim_X_train, dataset_1_sim_X_test, dataset_1_sim_y_train, dataset_1_sim_y_test = train_test_split(dataset_1_sim_X, dataset_1_sim_y, test_size=0.2, stratify = dataset_1_sim_y)
np.save(MAIN_PATH+"dataset/django_sim_X_train.npy",np.array(dataset_1_sim_X_train))
np.save(MAIN_PATH+"dataset/django_sim_X_test.npy",np.array(dataset_1_sim_X_test))
np.save(MAIN_PATH+"dataset/django_sim_y_train.npy",np.array(dataset_1_sim_y_train).flatten())
np.save(MAIN_PATH+"dataset/django_sim_y_test.npy",np.array(dataset_1_sim_y_test).flatten())
# dataset_1_sim_X_train_back = dataset_1_sim_X_train.copy()
# dataset_1_sim_X_test_back = dataset_1_sim_X_test.copy()

# verify the class distribution
print("class 0:1 in dataset_1_sim_y is       : {}:{}".format(Counter(list(dataset_1_sim_y['is_cloned']))[0]/dataset_1_sim_y.shape[0], 1-Counter(list(dataset_1_sim_y['is_cloned']))[0]/dataset_1_sim_y.shape[0]))
print("class 0:1 in dataset_1_sim_y_train is : {}:{}".format(Counter(list(dataset_1_sim_y_train['is_cloned']))[0]/dataset_1_sim_y_train.shape[0], 1-Counter(list(dataset_1_sim_y_train['is_cloned']))[0]/dataset_1_sim_y_train.shape[0]))
print("class 0:1 in dataset_1_sim_y_test is  : {}:{}".format(Counter(list(dataset_1_sim_y_test['is_cloned']))[0]/dataset_1_sim_y_test.shape[0], 1-Counter(list(dataset_1_sim_y_test['is_cloned']))[0]/dataset_1_sim_y_test.shape[0]))
print()

# the train set will be further split by stratified splitting for cross validation
skf = StratifiedKFold(n_splits=5)
skf_tr_va_sets = skf.split(dataset_1_sim_X_train, dataset_1_sim_y_train)
for idx_f, (idx_tr, idx_va) in enumerate(skf_tr_va_sets):
  tr_X = dataset_1_sim_X.iloc[idx_tr]
  tr_y = dataset_1_sim_y.iloc[idx_tr]
  va_X = dataset_1_sim_X.iloc[idx_va]
  va_y = dataset_1_sim_y.iloc[idx_va]
  print("Fold {}\ntrain set X:{} train set y: {} validation set X:{} validation set y: {}".format(idx_f,tr_X.shape,tr_y.shape,va_X.shape,va_y.shape))
  print("class 0:1 in train set y is      : {}:{}".format(Counter(list(tr_y['is_cloned']))[0]/tr_y.shape[0], 1-Counter(list(tr_y['is_cloned']))[0]/tr_y.shape[0]))
  print("class 0:1 in validation set y is : {}:{}".format(Counter(list(va_y['is_cloned']))[0]/va_y.shape[0], 1-Counter(list(va_y['is_cloned']))[0]/va_y.shape[0]))

"""## 4.13 Train and test

### 4.13.1 DT
"""

save_path = MAIN_PATH + "model/"
data_type = "django_sim"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='dt')

"""### 4.13.2 RF"""

save_path = MAIN_PATH + "model/"
data_type = "django_sim"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='rf')

"""### 4.13.3 KNN"""

save_path = MAIN_PATH + "model/"
data_type = "django_sim"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='knn')

"""### 4.13.4 NB"""

save_path = MAIN_PATH + "model/"
data_type = "django_sim"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nb')

"""### 4.13.5 LR"""

save_path = MAIN_PATH + "model/"
data_type = "django_sim"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lr')

"""### 4.13.6 LDA"""

save_path = MAIN_PATH + "model/"
data_type = "django_sim"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lda')

"""### 4.13.7 QDA"""

save_path = MAIN_PATH + "model/"
data_type = "django_sim"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='qda')

"""### 4.13.8 SVM"""

save_path = MAIN_PATH + "model/"
data_type = "django_sim"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='svm')

"""### 4.13.9 MLP"""

save_path = MAIN_PATH + "model/"
data_type = "django_sim"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='mlp')

"""### 4.13.10 NN"""

save_path = MAIN_PATH + "model/"
data_type = "django_sim"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nn')

"""## 4.14 Ensemble"""

save_path = MAIN_PATH + "model/"
data_type = "django"
metrics = os.listdir(save_path)

# select classifiers by thresholds
th_acc = 0.7
th_f1s = 0.7
clf_sel = []

print("Results:")
print("-"*80)
metrics_title = ['Accuracy','F1-score','Precision','Recall','Confusion Matrix']
print(" "*45,end="")
for m_t in metrics_title:
  print("{: >11} ".format(m_t),end="")
print()
# list label encoding classifiers
idx_clf = 0
for me in metrics:
  if "metric" in me:
    if data_type in me:    # model_le_smote_svm.pkl, true_le_smote_svm.npy, pred_le_smote_svm.npy, metric_le_smote_svm.npy
      idx_clf += 1
      clf_name = me[len("metric_"):len(me)-len(".npy")] 
      print("Classifier #{:2d}: {:20s} ".format(idx_clf, clf_name), end="")
      me_list = np.load(save_path+me)
      me_list = me_list.tolist()
      print("Metrics:",end="")
      for m_l in me_list:
        print("{: >11} ".format("{:4.7f}".format(m_l)),end="")
      print()

      # select the classifier if it meets thresholds
      if me_list[0]>=th_acc and me_list[1]>=th_f1s:
        clf_sel.append(clf_name)

print("{} classifiers selected:{}\n".format(len(clf_sel),clf_sel))
if len(clf_sel) > 0:
  # output the true and predictions of each selected classifier
  for i in range(len(clf_sel)):
    print("classifier #{:2d}".format(i+1))
    print("True      :",np.load(save_path+"true_"+clf_sel[i]+".npy").astype(np.uint8).tolist())
    print("Prediction:",np.load(save_path+"pred_"+clf_sel[i]+".npy").astype(np.uint8).tolist())

  # check whether the saved true values are the same as the original values
  flask_y_test = np.load(MAIN_PATH+"dataset/"+"flask_y_test.npy")

  # check whether each saved y values match the label encoding y values
  for i in range(len(clf_sel)):
    true = np.load(save_path+"true_"+clf_sel[i]+".npy")
    for j in range(len(true)):
      if list(flask_y_test)[j] - true.flatten()[j] !=0:
        print("y values of label encoding does not match those of saved y values for {}".format(clf_sel[i]))
        break
      elif j == len(true)-1:
        print("y values of label encoding match those of saved y vaues for {}".format(clf_sel[i]))  
      
  # generate the prediction values by voting
  pred = []
  for i in range(len(clf_sel)):
    pred.append(np.load(save_path+"pred_"+clf_sel[i]+".npy").astype(np.uint8).tolist())
  pred = np.array(pred)
  pred_ensemble = []
  for i in range(pred.shape[1]):
    if sum(pred[:,i])>=pred.shape[0]-sum(pred[:,i]):
      pred_ensemble.append(1)
    else:
      pred_ensemble.append(0)
  true = np.load(save_path+"true_"+clf_sel[0]+".npy").astype(np.uint8).tolist()
  print("True                :",true)
  print("Prediction by voting:",pred_ensemble)

  print("Accuracy score             :",accuracy_score(true, pred_ensemble))
  print("F1-score                   :",f1_score(true, pred_ensemble))
  print("Precision score            :",precision_score(true, pred_ensemble))
  print("Recall score               :",recall_score(true, pred_ensemble))
  print("Classification report      :\n",classification_report(true, pred_ensemble))
  print("Confusion Matrix           :\n",confusion_matrix(true, pred_ensemble))

  # plot confusion matrix
  cm = confusion_matrix(true, pred_ensemble)
  plot_confusion_matrix(cm, normalize = True, target_names = ["not cloned","cloned"], title = "Confusion Matrix")

else:
  print("No reliable results!")

"""## 4.15 Create new dataset - by Strategy 3: Feature Cross of the important features"""

# load dataframe
dataframe_django_path = "/content/drive/MyDrive/Software_data/code_metrics/dataframe_django.pkl"
dataframe_django = pd.read_pickle(dataframe_django_path)

dataframe_django.head()

# the important features of the original dataset were found to be 'CountStmtExe', 'CyclomaticStrict', 'CountLine', 'MaxNesting'
dataframe_django_cross = dataframe_django.iloc[:,:-1].copy()
important_feature = ['CountStmtExe', 'CyclomaticStrict', 'CountLine', 'MaxNesting']

# use the type1 method: multiply either two features
for idx_1 in range(len(important_feature)-1):
  for idx_2 in range(idx_1+1,len(important_feature)):
    # print(important_feature[idx_1],important_feature[idx_2])
    dataframe_django_cross[important_feature[idx_1]+"*"+important_feature[idx_2]] = dataframe_django_cross.apply(lambda row: row[important_feature[idx_1]]*row[important_feature[idx_2]], axis=1)

# use the type2 method: multiply all the features
dataframe_tmp = dataframe_django_cross[important_feature].copy()
dataframe_django_cross["*".join(important_feature)] = dataframe_tmp.prod(axis=1)

# use the type3 method: square each feature
for idx in range(len(important_feature)):
  dataframe_django_cross[important_feature[idx]+"**2"] = dataframe_django_cross.apply(lambda row: row[important_feature[idx]]**2, axis=1)

dataframe_django_cross['is_cloned'] = dataframe_django.iloc[:,-1:].copy()

dataframe_django_cross.head()

"""## 4.16 Split and save new train and test sets"""

# count the values occurrence in each column
for _ in dataframe_django_cross.columns.tolist():
  print(_)
  print(Counter(dataframe_django_cross[_]))

# save and load dataframe
dataframe_django_cross_path = "/content/drive/MyDrive/Software_data/code_metrics/dataframe_django_cross.pkl"

dataframe_django_cross.to_pickle(dataframe_django_cross_path) 

dataset_1 = pd.read_pickle(dataframe_django_cross_path)

dataset_1 = shuffle(dataset_1)
dataset_1_cross_X = dataset_1.iloc[:,0:dataset_1.shape[1]-1]
print("dataset_1_cross_X has a shape of {}".format(dataset_1_cross_X.shape))
print("the first 5 instances:\n{}".format(dataset_1_cross_X.iloc[0:5,:]))
dataset_1_cross_y = dataset_1.iloc[:,-1:]
print("dataset_1_cross_y has a shape of {}".format(dataset_1_cross_y.shape))
print("the first 5 instances:\n{}".format(dataset_1_cross_y.iloc[0:5,:]))
print()

# split into train test sets
dataset_1_cross_X_train, dataset_1_cross_X_test, dataset_1_cross_y_train, dataset_1_cross_y_test = train_test_split(dataset_1_cross_X, dataset_1_cross_y, test_size=0.2, stratify = dataset_1_cross_y)
np.save(MAIN_PATH+"dataset/django_cross_X_train.npy",np.array(dataset_1_cross_X_train))
np.save(MAIN_PATH+"dataset/django_cross_X_test.npy",np.array(dataset_1_cross_X_test))
np.save(MAIN_PATH+"dataset/django_cross_y_train.npy",np.array(dataset_1_cross_y_train).flatten())
np.save(MAIN_PATH+"dataset/django_cross_y_test.npy",np.array(dataset_1_cross_y_test).flatten())
# dataset_1_cross_X_train_back = dataset_1_cross_X_train.copy()
# dataset_1_cross_X_test_back = dataset_1_cross_X_test.copy()

# verify the class distribution
print("class 0:1 in dataset_1_cross_y is       : {}:{}".format(Counter(list(dataset_1_cross_y['is_cloned']))[0]/dataset_1_cross_y.shape[0], 1-Counter(list(dataset_1_cross_y['is_cloned']))[0]/dataset_1_cross_y.shape[0]))
print("class 0:1 in dataset_1_cross_y_train is : {}:{}".format(Counter(list(dataset_1_cross_y_train['is_cloned']))[0]/dataset_1_cross_y_train.shape[0], 1-Counter(list(dataset_1_cross_y_train['is_cloned']))[0]/dataset_1_cross_y_train.shape[0]))
print("class 0:1 in dataset_1_cross_y_test is  : {}:{}".format(Counter(list(dataset_1_cross_y_test['is_cloned']))[0]/dataset_1_cross_y_test.shape[0], 1-Counter(list(dataset_1_cross_y_test['is_cloned']))[0]/dataset_1_cross_y_test.shape[0]))
print()

# the train set will be further split by stratified splitting for cross validation
skf = StratifiedKFold(n_splits=5)
skf_tr_va_sets = skf.split(dataset_1_cross_X_train, dataset_1_cross_y_train)
for idx_f, (idx_tr, idx_va) in enumerate(skf_tr_va_sets):
  tr_X = dataset_1_cross_X.iloc[idx_tr]
  tr_y = dataset_1_cross_y.iloc[idx_tr]
  va_X = dataset_1_cross_X.iloc[idx_va]
  va_y = dataset_1_cross_y.iloc[idx_va]
  print("Fold {}\ntrain set X:{} train set y: {} validation set X:{} validation set y: {}".format(idx_f,tr_X.shape,tr_y.shape,va_X.shape,va_y.shape))
  print("class 0:1 in train set y is      : {}:{}".format(Counter(list(tr_y['is_cloned']))[0]/tr_y.shape[0], 1-Counter(list(tr_y['is_cloned']))[0]/tr_y.shape[0]))
  print("class 0:1 in validation set y is : {}:{}".format(Counter(list(va_y['is_cloned']))[0]/va_y.shape[0], 1-Counter(list(va_y['is_cloned']))[0]/va_y.shape[0]))

"""## 4.17 Train and test

### 4.17.1 DT
"""

save_path = MAIN_PATH + "model/"
data_type = "django_cross"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='dt')

"""### 4.17.2 RF"""

save_path = MAIN_PATH + "model/"
data_type = "django_cross"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='rf')

"""### 4.17.3 KNN"""

save_path = MAIN_PATH + "model/"
data_type = "django_cross"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='knn')

"""### 4.17.4 NB"""

save_path = MAIN_PATH + "model/"
data_type = "django_cross"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nb')

"""### 4.17.5 LR"""

save_path = MAIN_PATH + "model/"
data_type = "django_cross"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lr')

"""### 4.17.6 LDA"""

save_path = MAIN_PATH + "model/"
data_type = "django_cross"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='lda')

"""### 4.17.7 QDA"""

save_path = MAIN_PATH + "model/"
data_type = "django_cross"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='qda')

"""### 4.17.8 SVM"""

save_path = MAIN_PATH + "model/"
data_type = "django_cross"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='svm')

"""### 4.17.9 MLP"""

save_path = MAIN_PATH + "model/"
data_type = "django_cross"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='mlp')

"""### 4.17.10 NN"""

save_path = MAIN_PATH + "model/"
data_type = "django_cross"
X_train = np.load(MAIN_PATH+"dataset/"+data_type+"_X_train.npy")
y_train = np.load(MAIN_PATH+"dataset/"+data_type+"_y_train.npy")
X_test = np.load(MAIN_PATH+"dataset/"+data_type+"_X_test.npy")
y_test = np.load(MAIN_PATH+"dataset/"+data_type+"_y_test.npy")

train_model(X_train,y_train,X_test,y_test,save_path=save_path,data_type=data_type,model_type='nn')

"""## 4.18 Ensemble"""

save_path = MAIN_PATH + "model/"
data_type = "django"
metrics = os.listdir(save_path)

# select classifiers by thresholds
th_acc = 0.7
th_f1s = 0.7
clf_sel = []

print("Results:")
print("-"*80)
metrics_title = ['Accuracy','F1-score','Precision','Recall','Confusion Matrix']
print(" "*45,end="")
for m_t in metrics_title:
  print("{: >11} ".format(m_t),end="")
print()
# list label encoding classifiers
idx_clf = 0
for me in metrics:
  if "metric" in me:
    if data_type in me:    # model_le_smote_svm.pkl, true_le_smote_svm.npy, pred_le_smote_svm.npy, metric_le_smote_svm.npy
      idx_clf += 1
      clf_name = me[len("metric_"):len(me)-len(".npy")] 
      print("Classifier #{:2d}: {:20s} ".format(idx_clf, clf_name), end="")
      me_list = np.load(save_path+me)
      me_list = me_list.tolist()
      print("Metrics:",end="")
      for m_l in me_list:
        print("{: >11} ".format("{:4.7f}".format(m_l)),end="")
      print()

      # select the classifier if it meets thresholds
      if me_list[0]>=th_acc and me_list[1]>=th_f1s:
        clf_sel.append(clf_name)

print("{} classifiers selected:{}\n".format(len(clf_sel),clf_sel))
if len(clf_sel) > 0:
  # output the true and predictions of each selected classifier
  for i in range(len(clf_sel)):
    print("classifier #{:2d}".format(i+1))
    print("True      :",np.load(save_path+"true_"+clf_sel[i]+".npy").astype(np.uint8).tolist())
    print("Prediction:",np.load(save_path+"pred_"+clf_sel[i]+".npy").astype(np.uint8).tolist())

  # check whether the saved true values are the same as the original values
  flask_y_test = np.load(MAIN_PATH+"dataset/"+"flask_y_test.npy")

  # check whether each saved y values match the label encoding y values
  for i in range(len(clf_sel)):
    true = np.load(save_path+"true_"+clf_sel[i]+".npy")
    for j in range(len(true)):
      if list(flask_y_test)[j] - true.flatten()[j] !=0:
        print("y values of label encoding does not match those of saved y values for {}".format(clf_sel[i]))
        break
      elif j == len(true)-1:
        print("y values of label encoding match those of saved y vaues for {}".format(clf_sel[i]))  
      
  # generate the prediction values by voting
  pred = []
  for i in range(len(clf_sel)):
    pred.append(np.load(save_path+"pred_"+clf_sel[i]+".npy").astype(np.uint8).tolist())
  pred = np.array(pred)
  pred_ensemble = []
  for i in range(pred.shape[1]):
    if sum(pred[:,i])>=pred.shape[0]-sum(pred[:,i]):
      pred_ensemble.append(1)
    else:
      pred_ensemble.append(0)
  true = np.load(save_path+"true_"+clf_sel[0]+".npy").astype(np.uint8).tolist()
  print("True                :",true)
  print("Prediction by voting:",pred_ensemble)

  print("Accuracy score             :",accuracy_score(true, pred_ensemble))
  print("F1-score                   :",f1_score(true, pred_ensemble))
  print("Precision score            :",precision_score(true, pred_ensemble))
  print("Recall score               :",recall_score(true, pred_ensemble))
  print("Classification report      :\n",classification_report(true, pred_ensemble))
  print("Confusion Matrix           :\n",confusion_matrix(true, pred_ensemble))

  # plot confusion matrix
  cm = confusion_matrix(true, pred_ensemble)
  plot_confusion_matrix(cm, normalize = True, target_names = ["not cloned","cloned"], title = "Confusion Matrix")

else:
  print("No reliable results!")